{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.special  # for hyp2f1\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.618003Z","iopub.execute_input":"2025-03-26T08:38:08.618380Z","iopub.status.idle":"2025-03-26T08:38:08.623446Z","shell.execute_reply.started":"2025-03-26T08:38:08.618350Z","shell.execute_reply":"2025-03-26T08:38:08.622210Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# 1) BG/NBD Model (Fully Implemented: Log-Likelihood + Expected Transactions)\n\nclass BGNBDModel(nn.Module):\n    \"\"\"\n    BG/NBD model for transaction frequency:\n      - r, alpha : Gamma-Poisson mixture for transaction rates\n      - a, b     : Beta-Geometric mixture for dropout\n    We implement:\n      forward(x,t_x,T) -> log-likelihood for each customer\n      expected_transactions(...) -> closed-form approximation of E[# of future txns]\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(BGNBDModel, self).__init__()\n        if init_params is None:\n            init_params = {'r': 1.0, 'alpha': 1.0, 'a': 1.0, 'b': 1.0}\n        self.log_r = nn.Parameter(torch.log(torch.tensor(init_params['r'], dtype=torch.float32)))\n        self.log_alpha = nn.Parameter(torch.log(torch.tensor(init_params['alpha'], dtype=torch.float32)))\n        self.log_a = nn.Parameter(torch.log(torch.tensor(init_params['a'], dtype=torch.float32)))\n        self.log_b = nn.Parameter(torch.log(torch.tensor(init_params['b'], dtype=torch.float32)))\n\n    def forward(self, x, t_x, T):\n        \"\"\"\n        Return the log-likelihood (LL) for each customer under BG/NBD.\n\n        If x == 0:\n          log L(0,0,T) = r*log(alpha) - r*log(alpha + T) + log(b) - log(a+b)\n\n        If x > 0:\n          log L(x,t_x,T) =\n             + logGamma(r+x) - logGamma(r) - logGamma(x+1)\n             + r [log(alpha) - log(alpha+T)]\n             + x [log(T - t_x) - log(alpha+T)]\n             + log(a) + logGamma(a+b) - logGamma(a) - logGamma(a+b+x) + logGamma(a+x)\n             + log( 2F1(r+x, a; a+b+x; (T - t_x)/(alpha+T)) )\n        \"\"\"\n        # Ensure float\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        # Recover parameters\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        ll = torch.empty_like(x)\n\n        mask0 = (x == 0)\n        mask1 = (x > 0)\n\n        # --- x=0 case ---\n        if mask0.any():\n            T0 = T[mask0]\n            ll0 = (r * torch.log(alpha)\n                   - r * torch.log(alpha + T0)\n                   + torch.log(b)\n                   - torch.log(a + b))\n            ll[mask0] = ll0\n\n        # --- x>0 case ---\n        if mask1.any():\n            x1   = x[mask1]\n            t_x1 = t_x[mask1]\n            T1   = T[mask1]\n\n            term1 = ( torch.lgamma(r + x1)\n                      - torch.lgamma(r)\n                      - torch.lgamma(x1 + 1) )\n            term2 = r * ( torch.log(alpha) - torch.log(alpha + T1) )\n            term3 = x1 * ( torch.log(T1 - t_x1) - torch.log(alpha + T1) )\n            term4 = ( torch.log(a)\n                      + torch.lgamma(a + b)\n                      - torch.lgamma(a)\n                      - torch.lgamma(a + b + x1)\n                      + torch.lgamma(a + x1) )\n            z = (T1 - t_x1) / (alpha + T1)\n            # hypergeometric\n            hyp_val = torch.special.hyp2f1(r + x1, a, a + b + x1, z)\n            term5 = torch.log(hyp_val + 1e-30)  # add small epsilon for safety\n\n            ll1 = term1 + term2 + term3 + term4 + term5\n            ll[mask1] = ll1\n\n        return ll\n\n    def negative_log_likelihood(self, x, t_x, T):\n        \"\"\"Sum of negative log-likelihood over all customers.\"\"\"\n        ll = self.forward(x, t_x, T)\n        return -ll.sum()\n\n    def expected_transactions(self, x, t_x, T, t_future=10.0):\n        \"\"\"\n        Predict E[# of transactions in (T, T + t_future)] for a customer with\n        history (x, t_x, T) under BG/NBD. We'll use a known approximation:\n\n          1) Probability that the customer is alive at time T:\n             p_alive = 1 / [ 1 + factor ]\n             where factor = exp( log_factor ) as in Hardie notes.\n\n          2) E[X(t_future)| x,t_x,T] ~ p_alive * ((r + x)/(alpha + T)) * t_future\n             (This is a common simplified expression for BG/NBD.)\n\n        For more exact formulas, see Hardie & Fader (2005).\n        \"\"\"\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        # Probability alive\n        # log_factor = logGamma(a+1) + logGamma(b+x) - logGamma(a) - logGamma(b+x+1)\n        #           + (r+x)*[ log(alpha+T) - log(alpha+t_x) ]\n        log_factor = ( torch.lgamma(a + 1)\n                       + torch.lgamma(b + x)\n                       - torch.lgamma(a)\n                       - torch.lgamma(b + x + 1) )\n        log_factor += (r + x) * ( torch.log(alpha + T) - torch.log(alpha + t_x + 1e-8) )\n        factor = torch.exp(log_factor)\n        p_alive = 1.0 / (1.0 + factor)\n\n        # approximate expected future transactions\n        return p_alive * ( (r + x) / (alpha + T + 1e-8) ) * t_future\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.626075Z","iopub.execute_input":"2025-03-26T08:38:08.626488Z","iopub.status.idle":"2025-03-26T08:38:08.652248Z","shell.execute_reply.started":"2025-03-26T08:38:08.626455Z","shell.execute_reply":"2025-03-26T08:38:08.650796Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# 2) Gamma–Gamma Model (Fully Implemented)\n\nclass GammaGammaModel(nn.Module):\n    \"\"\"\n    Gamma–Gamma model for monetary value:\n      p, q, v (all stored in log-space).\n    forward() -> log-likelihood\n    conditional_expected_value(x,m) -> E(m|x,m).\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(GammaGammaModel, self).__init__()\n        if init_params is None:\n            init_params = {'p': 1.0, 'q': 1.0, 'v': 1.0}\n        self.log_p = nn.Parameter(torch.log(torch.tensor(init_params['p'], dtype=torch.float32)))\n        self.log_q = nn.Parameter(torch.log(torch.tensor(init_params['q'], dtype=torch.float32)))\n        self.log_v = nn.Parameter(torch.log(torch.tensor(init_params['v'], dtype=torch.float32)))\n\n    def forward(self, x, m):\n        \"\"\"\n        Return log-likelihood for each customer:\n          log L = logGamma(p + qx) - logGamma(p) - logGamma(qx)\n                  + p log v + (q - 1)x log m - (p + qx) log(v + x m)\n        Usually only applied to customers with x>0, but here we just do it for all.\n        \"\"\"\n        x = x.float()\n        m = m.float()\n\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n\n        # We'll define a small epsilon for safety\n        eps = 1e-30\n\n        term1 = torch.lgamma(p + q*x) - torch.lgamma(p) - torch.lgamma(q*x + eps)\n        term2 = p * torch.log(v + eps)\n        term3 = (q - 1) * x * torch.log(m + eps)\n        term4 = - (p + q*x) * torch.log(v + x*m + eps)\n\n        ll = term1 + term2 + term3 + term4\n        return ll\n\n    def negative_log_likelihood(self, x, m):\n        ll = self.forward(x, m)\n        return -ll.sum()\n\n    def conditional_expected_value(self, x, m):\n        \"\"\"\n        E(m| x, m) = (p + q*x) / (v + x*m).\n        \"\"\"\n        x = x.float()\n        m = m.float()\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n        eps = 1e-8\n        return (p + q * x) / (v + x*m + eps)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.653966Z","iopub.execute_input":"2025-03-26T08:38:08.654385Z","iopub.status.idle":"2025-03-26T08:38:08.675203Z","shell.execute_reply.started":"2025-03-26T08:38:08.654344Z","shell.execute_reply":"2025-03-26T08:38:08.673878Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# 3) Composite Model that Multiplies BG/NBD * Gamma–Gamma to Predict CLV\n\nclass CompositeCLVModel(nn.Module):\n    \"\"\"\n    - We train it by comparing predicted CLV to a known 'regression_label'.\n    - The forward pass = E[# future transactions] * E[monetary value].\n    \"\"\"\n    def __init__(self, init_bgnbd=None, init_gg=None, t_future=10.0):\n        super(CompositeCLVModel, self).__init__()\n        self.bgnbd = BGNBDModel(init_params=init_bgnbd)\n        self.ggamma = GammaGammaModel(init_params=init_gg)\n        self.t_future = t_future\n\n    def forward(self, x, t_x, T, m):\n        # predicted # future transactions\n        count_pred = self.bgnbd.expected_transactions(x, t_x, T, self.t_future)\n        # predicted average value\n        val_pred = self.ggamma.conditional_expected_value(x, m)\n        return count_pred * val_pred\n\n    def loss_mse(self, x, t_x, T, m, actual_spend):\n        \"\"\"\n        MSE vs. the actual future spend (regression_label).\n        \"\"\"\n        pred_spend = self.forward(x, t_x, T, m)\n        return torch.mean((pred_spend - actual_spend)**2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.677506Z","iopub.execute_input":"2025-03-26T08:38:08.678140Z","iopub.status.idle":"2025-03-26T08:38:08.702135Z","shell.execute_reply.started":"2025-03-26T08:38:08.678087Z","shell.execute_reply":"2025-03-26T08:38:08.700811Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# 4) Data Preparation: parse (x, t_x, T) from 'days_before_lst', parse m from 'articles_ids_lst'\n\ndef parse_x_t_x_T(row):\n    \"\"\"\n    Example parse from 'days_before_lst':\n       x = len(days_before_lst)\n       T = sum of days_before_lst\n       t_x = last element\n    \"\"\"\n    days_list = row['days_before_lst']\n    if not isinstance(days_list, list) or len(days_list) == 0:\n        return 0, 0.0, 0.0\n    else:\n        x = len(days_list)\n        T = float(sum(days_list))\n        t_x = float(days_list[-1])\n        return x, t_x, T\n\ndef parse_avg_monetary_value(row):\n    \"\"\"\n    Example parse from 'articles_ids_lst':\n    We'll define a dummy approach: average value = 20 + 0.1 * (#articles).\n    \"\"\"\n    arts = row['articles_ids_lst']\n    if not isinstance(arts, list) or len(arts) == 0:\n        return 20.0\n    else:\n        return 20.0 + 0.1*len(arts)\n\ndef build_tensor_dataset(df):\n    \"\"\"\n    Convert the DataFrame to PyTorch Tensors for (x, t_x, T, m, regression_label).\n    We dont use 'classification_label' or 'customer_id' or 'postal_code' as features.\n    \"\"\"\n    x_list, t_x_list, T_list, m_list, spend_list = [], [], [], [], []\n    for _, row in df.iterrows():\n        x_val, tx_val, bigT_val = parse_x_t_x_T(row)\n        m_val = parse_avg_monetary_value(row)\n        # future spend is stored in regression_label\n        clv = row.get('regression_label', 0.0)\n\n        x_list.append(x_val)\n        t_x_list.append(tx_val)\n        T_list.append(bigT_val)\n        m_list.append(m_val)\n        spend_list.append(clv)\n\n    x_ten = torch.tensor(x_list, dtype=torch.float32)\n    t_x_ten = torch.tensor(t_x_list, dtype=torch.float32)\n    T_ten = torch.tensor(T_list, dtype=torch.float32)\n    m_ten = torch.tensor(m_list, dtype=torch.float32)\n    spend_ten = torch.tensor(spend_list, dtype=torch.float32)\n\n    return x_ten, t_x_ten, T_ten, m_ten, spend_ten\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.704095Z","iopub.execute_input":"2025-03-26T08:38:08.704534Z","iopub.status.idle":"2025-03-26T08:38:08.731350Z","shell.execute_reply.started":"2025-03-26T08:38:08.704488Z","shell.execute_reply":"2025-03-26T08:38:08.730087Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# 5) Example Pipeline: train_and_validate\n\ndef train_and_validate(train_df, val_df, test_df):\n    \"\"\"\n    1) Build Tensors from each DataFrame\n    2) Create CompositeCLVModel\n    3) Train end-to-end to minimize MSE vs. regression_label\n    4) Evaluate on val/test sets\n    \"\"\"\n    # Convert to Tensors\n    x_train, t_x_train, T_train, m_train, spend_train = build_tensor_dataset(train_df)\n    x_val, t_x_val, T_val, m_val, spend_val = build_tensor_dataset(val_df)\n    x_test, t_x_test, T_test, m_test, spend_test = build_tensor_dataset(test_df)\n\n    # Create the composite model\n    model = CompositeCLVModel(\n        init_bgnbd={'r':1.0, 'alpha':1.0, 'a':1.0, 'b':1.0},\n        init_gg={'p':1.0, 'q':1.0, 'v':1.0},\n        t_future=10.0  # horizon for future transactions\n    )\n    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n\n    n_epochs = 500\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        loss = model.loss_mse(x_train, t_x_train, T_train, m_train, spend_train)\n        loss.backward()\n        optimizer.step()\n\n        # check validation\n        if (epoch+1) % 100 == 0:\n            with torch.no_grad():\n                val_preds = model.forward(x_val, t_x_val, T_val, m_val)\n                val_loss = torch.mean((val_preds - spend_val)**2)\n            print(f\"Epoch {epoch+1}/{n_epochs}, Train MSE={loss.item():.4f}, Val MSE={val_loss.item():.4f}\")\n\n    # Final test evaluation\n    with torch.no_grad():\n        test_preds = model.forward(x_test, t_x_test, T_test, m_test)\n        test_mse = torch.mean((test_preds - spend_test)**2).item()\n    print(f\"\\n=== Final Test MSE: {test_mse:.4f} ===\")\n\n    # Print learned parameters\n    print(\"\\nLearned BG/NBD parameters:\")\n    print(\"r     =\", torch.exp(model.bgnbd.log_r).item())\n    print(\"alpha =\", torch.exp(model.bgnbd.log_alpha).item())\n    print(\"a     =\", torch.exp(model.bgnbd.log_a).item())\n    print(\"b     =\", torch.exp(model.bgnbd.log_b).item())\n\n    print(\"\\nLearned Gamma–Gamma parameters:\")\n    print(\"p =\", torch.exp(model.ggamma.log_p).item())\n    print(\"q =\", torch.exp(model.ggamma.log_q).item())\n    print(\"v =\", torch.exp(model.ggamma.log_v).item())\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.733169Z","iopub.execute_input":"2025-03-26T08:38:08.733524Z","iopub.status.idle":"2025-03-26T08:38:08.757188Z","shell.execute_reply.started":"2025-03-26T08:38:08.733493Z","shell.execute_reply":"2025-03-26T08:38:08.755909Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"##############################################################################\n# 6) DEMO\n##############################################################################\n\n# train_and_validate(train_df, val_df, test_df)\n\n\nif __name__ == \"__main__\":\n    # Minimal dummy data\n    train_df = pd.DataFrame({\n        'customer_id': ['C1','C2','C3'],\n        'days_before_lst': [[10,20],[30],[15,5,5]],\n        'articles_ids_lst': [[111,112],[999],[50,51,52]],\n        'regression_label': [100.0, 30.0, 120.0],\n        'classification_label': [1, 0, 1],  # not used\n        'age': [25, 40, 35],               # not used here\n        'postal_code': ['xyz','abc','def'] # not used\n    })\n    val_df = pd.DataFrame({\n        'customer_id': ['C4'],\n        'days_before_lst': [[10]],\n        'articles_ids_lst': [[1234]],\n        'regression_label': [50.0],\n        'classification_label': [0],\n        'age': [45],\n        'postal_code': ['zzz']\n    })\n    test_df = pd.DataFrame({\n        'customer_id': ['C5'],\n        'days_before_lst': [[5,5]],\n        'articles_ids_lst': [[9999,10000]],\n        'regression_label': [80.0],\n        'classification_label': [1],\n        'age': [50],\n        'postal_code': ['qqq']\n    })\n\n    print(\"=== Training on dummy data ===\")\n    train_and_validate(train_df, val_df, test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:38:08.758358Z","iopub.execute_input":"2025-03-26T08:38:08.758788Z","iopub.status.idle":"2025-03-26T08:38:09.684784Z","shell.execute_reply.started":"2025-03-26T08:38:08.758714Z","shell.execute_reply":"2025-03-26T08:38:09.683815Z"}},"outputs":[{"name":"stdout","text":"=== Training on dummy data ===\nEpoch 100/500, Train MSE=8398.9580, Val MSE=2330.1304\nEpoch 200/500, Train MSE=7561.3081, Val MSE=11.1135\nEpoch 300/500, Train MSE=5743.3223, Val MSE=11892.7354\nEpoch 400/500, Train MSE=716.7192, Val MSE=9693.3721\nEpoch 500/500, Train MSE=482.7324, Val MSE=8146.9438\n\n=== Final Test MSE: 10533.6191 ===\n\nLearned BG/NBD parameters:\nr     = 2.894625425338745\nalpha = 5.774572372436523\na     = 0.0032321808394044638\nb     = 383.3000793457031\n\nLearned Gamma–Gamma parameters:\np = 44.45631408691406\nq = 1241.3653564453125\nv = 2.5334112644195557\n","output_type":"stream"}],"execution_count":8}]}
