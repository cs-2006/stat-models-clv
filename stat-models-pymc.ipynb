{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11087754,"sourceType":"datasetVersion","datasetId":6142016}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTALL REQUIREMENTS","metadata":{"_uuid":"055c2294-0da3-48ba-a5fe-4237081586cd","_cell_guid":"94e01011-a65b-4bb0-b0d9-fd3cdf0a3a71","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirements1.txt","metadata":{"_uuid":"57e12ed4-e815-47d6-9e64-a5e11ba7e9c3","_cell_guid":"465371ab-7beb-46c3-896e-d7e098e1f993","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:00:21.472305Z","iopub.execute_input":"2025-04-09T08:00:21.472559Z","iopub.status.idle":"2025-04-09T08:01:20.302735Z","shell.execute_reply.started":"2025-04-09T08:00:21.472524Z","shell.execute_reply":"2025-04-09T08:01:20.301935Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting appnope==0.1.4 (from -r /kaggle/input/requirements1.txt (line 1))\n  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\nRequirement already satisfied: asttokens==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 2)) (3.0.0)\nCollecting black==24.10.0 (from -r /kaggle/input/requirements1.txt (line 3))\n  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: catboost==1.2.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 4)) (1.2.7)\nRequirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 5)) (8.1.7)\nRequirement already satisfied: comm==0.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 6)) (0.2.2)\nRequirement already satisfied: contourpy==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 7)) (1.3.1)\nCollecting cramjam==2.9.0 (from -r /kaggle/input/requirements1.txt (line 8))\n  Downloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 9)) (0.12.1)\nCollecting debugpy==1.6.7 (from -r /kaggle/input/requirements1.txt (line 10))\n  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting decorator==5.1.1 (from -r /kaggle/input/requirements1.txt (line 11))\n  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: exceptiongroup==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 12)) (1.2.2)\nCollecting executing==2.1.0 (from -r /kaggle/input/requirements1.txt (line 13))\n  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\nCollecting fastparquet==2024.11.0 (from -r /kaggle/input/requirements1.txt (line 14))\n  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting filelock==3.16.1 (from -r /kaggle/input/requirements1.txt (line 15))\n  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting fonttools==4.55.1 (from -r /kaggle/input/requirements1.txt (line 16))\n  Downloading fonttools-4.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec==2024.10.0 (from -r /kaggle/input/requirements1.txt (line 17))\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: graphviz==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 18)) (0.20.3)\nRequirement already satisfied: importlib_metadata==8.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 19)) (8.5.0)\nCollecting ipykernel==6.29.5 (from -r /kaggle/input/requirements1.txt (line 20))\n  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting ipython==8.30.0 (from -r /kaggle/input/requirements1.txt (line 21))\n  Downloading ipython-8.30.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: jedi==0.19.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 22)) (0.19.2)\nRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 23)) (3.1.4)\nRequirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 24)) (1.4.2)\nRequirement already satisfied: jupyter_client==8.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 25)) (8.6.3)\nRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 26)) (5.7.2)\nRequirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 27)) (1.4.7)\nRequirement already satisfied: lightgbm==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 28)) (4.5.0)\nRequirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 29)) (3.0.2)\nCollecting matplotlib==3.9.3 (from -r /kaggle/input/requirements1.txt (line 30))\n  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 31)) (0.1.7)\nRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 32)) (1.3.0)\nRequirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 33)) (1.0.0)\nRequirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 34)) (1.6.0)\nRequirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 35)) (3.4.2)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 36)) (1.26.4)\nRequirement already satisfied: packaging==24.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 37)) (24.2)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 38)) (2.2.3)\nRequirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 39)) (0.8.4)\nCollecting pathspec==0.12.1 (from -r /kaggle/input/requirements1.txt (line 40))\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 41)) (4.9.0)\nRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 42)) (0.7.5)\nRequirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 43)) (11.0.0)\nCollecting pip==24.2 (from -r /kaggle/input/requirements1.txt (line 44))\n  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 45)) (4.3.6)\nRequirement already satisfied: plotly==5.24.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 46)) (5.24.1)\nCollecting polars==0.20.15 (from -r /kaggle/input/requirements1.txt (line 47))\n  Downloading polars-0.20.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: prompt_toolkit==3.0.48 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 48)) (3.0.48)\nCollecting psutil==5.9.0 (from -r /kaggle/input/requirements1.txt (line 49))\n  Downloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 50)) (0.7.0)\nCollecting pure_eval==0.2.3 (from -r /kaggle/input/requirements1.txt (line 51))\n  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting Pygments==2.18.0 (from -r /kaggle/input/requirements1.txt (line 52))\n  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 53)) (3.2.0)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 54)) (2.9.0.post0)\nCollecting pytz==2024.2 (from -r /kaggle/input/requirements1.txt (line 55))\n  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting pyzmq==25.1.2 (from -r /kaggle/input/requirements1.txt (line 56))\n  Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting scikit-learn==1.5.2 (from -r /kaggle/input/requirements1.txt (line 57))\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting scipy==1.14.1 (from -r /kaggle/input/requirements1.txt (line 58))\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting seaborn==0.13.2 (from -r /kaggle/input/requirements1.txt (line 59))\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: setuptools==75.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 60)) (75.1.0)\nRequirement already satisfied: six==1.17.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 61)) (1.17.0)\nCollecting stack-data==0.6.2 (from -r /kaggle/input/requirements1.txt (line 62))\n  Downloading stack_data-0.6.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 63)) (1.13.1)\nRequirement already satisfied: tenacity==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 64)) (9.0.0)\nRequirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 65)) (3.5.0)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 66)) (2.5.1+cu121)\nCollecting tornado==6.4.1 (from -r /kaggle/input/requirements1.txt (line 67))\n  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 68)) (4.67.1)\nCollecting traitlets==5.14.3 (from -r /kaggle/input/requirements1.txt (line 69))\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 70)) (4.12.2)\nCollecting tzdata==2024.2 (from -r /kaggle/input/requirements1.txt (line 71))\n  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 72)) (0.2.13)\nCollecting wheel==0.44.0 (from -r /kaggle/input/requirements1.txt (line 73))\n  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting xgboost==2.1.3 (from -r /kaggle/input/requirements1.txt (line 74))\n  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: zipp==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 75)) (3.21.0)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==24.10.0->-r /kaggle/input/requirements1.txt (line 3)) (2.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2.4.1)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost==2.1.3->-r /kaggle/input/requirements1.txt (line 74)) (2.23.4)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nDownloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\nDownloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\nDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\nDownloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\nDownloading fonttools-4.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipython-8.30.0-py3-none-any.whl (820 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.8/820.8 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading polars-0.20.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stack_data-0.6.2-py3-none-any.whl (24 kB)\nDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pytz, pure_eval, wheel, tzdata, traitlets, tornado, pyzmq, Pygments, psutil, polars, pip, pathspec, fsspec, fonttools, filelock, executing, decorator, debugpy, cramjam, appnope, stack-data, black, ipython, ipykernel, scipy, matplotlib, xgboost, seaborn, scikit-learn, fastparquet\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.1\n    Uninstalling pytz-2025.1:\n      Successfully uninstalled pytz-2025.1\n  Attempting uninstall: wheel\n    Found existing installation: wheel 0.45.1\n    Uninstalling wheel-0.45.1:\n      Successfully uninstalled wheel-0.45.1\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.1\n    Uninstalling tzdata-2025.1:\n      Successfully uninstalled tzdata-2025.1\n  Attempting uninstall: traitlets\n    Found existing installation: traitlets 5.7.1\n    Uninstalling traitlets-5.7.1:\n      Successfully uninstalled traitlets-5.7.1\n  Attempting uninstall: tornado\n    Found existing installation: tornado 6.3.3\n    Uninstalling tornado-6.3.3:\n      Successfully uninstalled tornado-6.3.3\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: Pygments\n    Found existing installation: Pygments 2.19.1\n    Uninstalling Pygments-2.19.1:\n      Successfully uninstalled Pygments-2.19.1\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.5\n    Uninstalling psutil-5.9.5:\n      Successfully uninstalled psutil-5.9.5\n  Attempting uninstall: polars\n    Found existing installation: polars 1.9.0\n    Uninstalling polars-1.9.0:\n      Successfully uninstalled polars-1.9.0\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.12.0\n    Uninstalling fsspec-2024.12.0:\n      Successfully uninstalled fsspec-2024.12.0\n  Attempting uninstall: fonttools\n    Found existing installation: fonttools 4.55.3\n    Uninstalling fonttools-4.55.3:\n      Successfully uninstalled fonttools-4.55.3\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.17.0\n    Uninstalling filelock-3.17.0:\n      Successfully uninstalled filelock-3.17.0\n  Attempting uninstall: decorator\n    Found existing installation: decorator 4.4.2\n    Uninstalling decorator-4.4.2:\n      Successfully uninstalled decorator-4.4.2\n  Attempting uninstall: debugpy\n    Found existing installation: debugpy 1.8.0\n    Uninstalling debugpy-1.8.0:\n      Successfully uninstalled debugpy-1.8.0\n  Attempting uninstall: ipython\n    Found existing installation: ipython 7.34.0\n    Uninstalling ipython-7.34.0:\n      Successfully uninstalled ipython-7.34.0\n  Attempting uninstall: ipykernel\n    Found existing installation: ipykernel 5.5.6\n    Uninstalling ipykernel-5.5.6:\n      Successfully uninstalled ipykernel-5.5.6\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.13.1\n    Uninstalling scipy-1.13.1:\n      Successfully uninstalled scipy-1.13.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\ngoogle-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\ngoogle-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.30.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.3.3, but you have tornado 6.4.1 which is incompatible.\nmoviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pygments-2.18.0 appnope-0.1.4 black-24.10.0 cramjam-2.9.0 debugpy-1.6.7 decorator-5.1.1 executing-2.1.0 fastparquet-2024.11.0 filelock-3.16.1 fonttools-4.55.1 fsspec-2024.10.0 ipykernel-6.29.5 ipython-8.30.0 matplotlib-3.9.3 pathspec-0.12.1 pip-24.2 polars-0.20.15 psutil-5.9.0 pure_eval-0.2.3 pytz-2024.2 pyzmq-25.1.2 scikit-learn-1.5.2 scipy-1.14.1 seaborn-0.13.2 stack-data-0.6.2 tornado-6.4.1 traitlets-5.14.3 tzdata-2024.2 wheel-0.44.0 xgboost-2.1.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# DATA PROCESSING UTILS","metadata":{"_uuid":"57aaa480-97c5-4dab-bbd3-ecaddaa36db5","_cell_guid":"3b77815c-d72a-4246-8199-6c3068f072f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\n\n\nclass StatelessRandomGenerator:\n    def __init__(self, seed=42):\n        self.seed = seed\n\n    def set_seed(self, new_seed):\n        self.seed = new_seed\n\n    def random(self, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.random(size)\n\n    def integers(self, low, high=None, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.integers(low, high, size)\n\n    def choice(self, a, size=None, replace=True, p=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.choice(a, size, replace, p)\n\n\nglobal_rng = StatelessRandomGenerator(42)\n\n\ndef set_global_seed(new_seed):\n    global_rng.set_seed(new_seed)","metadata":{"_uuid":"6972ea52-b941-4eb3-a62d-4b3a4598c241","_cell_guid":"18ad3284-909f-45e2-8bc2-facfab6315e2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:01:50.329708Z","iopub.execute_input":"2025-04-09T08:01:50.330048Z","iopub.status.idle":"2025-04-09T08:01:50.336039Z","shell.execute_reply.started":"2025-04-09T08:01:50.330020Z","shell.execute_reply":"2025-04-09T08:01:50.335271Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\n\n\ndef wmape_metric(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n    return torch.sum(torch.abs(pred - true), dim=0) / torch.sum(true, dim=0)","metadata":{"_uuid":"704f44d9-3f2b-4145-836a-f80070d870a4","_cell_guid":"194b5a69-c6d2-4ecc-8496-f042a070eeab","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:01:53.136629Z","iopub.execute_input":"2025-04-09T08:01:53.136979Z","iopub.status.idle":"2025-04-09T08:01:56.598924Z","shell.execute_reply.started":"2025-04-09T08:01:53.136951Z","shell.execute_reply":"2025-04-09T08:01:56.598117Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# DATA PROCESSING","metadata":{"_uuid":"b61427f1-f86a-4acd-84b6-b3751252d702","_cell_guid":"360816e8-2a0a-4068-8517-14def4531823","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from datetime import datetime\nimport json\nfrom pathlib import Path\nimport polars as pl\n#from data_processing.utils.stateless_rng import global_rng\n\ndef filter_purchases_purchases_per_month_pl(\n    df_pl: pl.DataFrame, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Filters extreme customers and groups purchases by date and optionally by sales channel.\n\n    This function:\n    1. Groups transactions by customer, date, and optionally sales channel\n    2. Identifies extreme customers based on the 99th percentile of total items purchased\n    3. Removes these customers from the dataset\n\n    Args:\n        df_pl (pl.DataFrame): Input transaction dataframe containing:\n            - customer_id: Customer identifier\n            - date: Transaction date\n            - article_id: Product identifier\n            - price: Transaction price\n            - sales_channel_id: Sales channel identifier\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group transactions by sales channel. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Grouped transaction data with columns:\n                - customer_id, date, [sales_channel_id], article_ids, total_price, prices, num_items\n            - extreme_customers: DataFrame of customers identified as outliers based on purchase behavior\n\n    Notes:\n        Extreme customers are identified using the 99th percentile of total items purchased\n        during the training period.\n    \"\"\"\n    # Used for multi variate time series\n    if group_by_channel_id:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\", \"sales_channel_id\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n    else:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"sales_channel_id\").explode().alias(\"sales_channel_ids\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n\n    # Only remove customers with extreme purchases in train period\n    customers_summary = (\n        df_pl.lazy()\n        .filter(pl.col(\"date\") < train_end)\n        .group_by(\"customer_id\")\n        .agg(\n            [\n                pl.col(\"date\").n_unique().alias(\"total_purchases\"),\n                pl.col(\"price\").sum().round(2).alias(\"total_spent\"),\n                pl.col(\"article_id\").flatten().alias(\"flattened_ids\")\n            ]\n        )\n        .with_columns(pl.col(\"flattened_ids\").list.len().alias(\"total_items\"))\n    )\n\n    quantile = 0.99\n    total_purchases_99, total_spending_99, total_items_99 = (\n        customers_summary.select(\n            [\n                pl.col(\"total_purchases\").quantile(quantile),\n                pl.col(\"total_spent\").quantile(quantile),\n                pl.col(\"total_items\").quantile(quantile),\n            ]\n        )\n        .collect()\n        .to_numpy()\n        .flatten()\n    )\n\n    # Currently only remove customers with very large number of total items purchased\n    extreme_customers = customers_summary.filter(\n        (pl.col(\"total_items\") >= total_items_99)\n        # | (pl.col(\"total_purchases\") >= total_purchases_99)\n        # | (pl.col(\"total_spent\") >= total_spending_99)\n    )\n\n    extreme_customers = extreme_customers.select(\"customer_id\").unique()\n    extreme_customers = extreme_customers.collect()\n\n    print(\n        f\"\"\"\n        Cutoff Values for {quantile*100}th Percentiles:\n        -----------------------------------\n        Total items bought:    {total_items_99:.0f} items\n\n        -----------------------------------\n        Removed Customers:     {len(extreme_customers):,}\n        \"\"\"\n    )\n\n    return grouped_df.collect(), extreme_customers\n\ndef train_test_split(\n    train_df: pl.DataFrame,\n    test_df: pl.DataFrame,\n    subset: int = None,\n    train_subsample_percentage: float = None,\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits data into train, validation, and test sets with optional subsampling.\n\n    The function performs the following operations:\n    1. Optional subsampling of both train and test data\n    2. Optional percentage-based subsampling of training data\n    3. Creates a validation set from 10% of the training data\n\n    Args:\n        train_df (pl.DataFrame): Training dataset.\n        test_df (pl.DataFrame): Test dataset.\n        subset (int, optional): If provided, limits both train and test sets to first n rows. \n            Defaults to None.\n        train_subsample_percentage (float, optional): If provided, randomly samples this percentage \n            of training data. Defaults to None.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (90% of training data after subsampling)\n            - val_df: Validation dataset (10% of training data)\n            - test_df: Test dataset (potentially subsampled)\n\n    Notes:\n        If both subset and train_subsample_percentage are provided, subset is applied first.\n        The validation set is always 10% of the remaining training data after any subsampling.\n    \"\"\"\n\n    if subset is not None:\n        train_df = train_df[:subset]\n        test_df = test_df[:subset]\n    elif train_subsample_percentage is not None:\n        sampled_indices = global_rng.choice(\n            len(train_df),\n            size=int(train_subsample_percentage * len(train_df)),\n            replace=False,\n        )\n        train_df = train_df[sampled_indices]\n\n    # Train-val-split\n    # Calculate 10% of the length of the array\n    sampled_indices = global_rng.choice(\n        len(train_df), size=int(0.1 * len(train_df)), replace=False\n    )\n    val_df = train_df[sampled_indices]\n    train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n\n    return train_df, val_df, test_df\n\ndef map_article_ids(df: pl.DataFrame, data_path: Path) -> pl.DataFrame:\n    \"\"\"Maps article IDs to new running IDs using a mapping dictionary from JSON.\n\n    Args:\n        df (pl.DataFrame): DataFrame with 'article_id' column to be mapped.\n        data_path (Path): Path to directory with 'running_id_dict.json' containing ID mappings.\n\n    Returns:\n        pl.DataFrame: DataFrame with mapped article IDs, sorted by new IDs. Non-mapped articles are removed.\n    \"\"\"\n    with open(data_path / \"running_id_dict.json\", \"r\") as f:\n        data = json.load(f)\n    article_id_dict = data[\"combined\"]\n\n    mapping_df = pl.DataFrame(\n        {\n            \"old_id\": list(article_id_dict.keys()),\n            \"new_id\": list(article_id_dict.values()),\n        },\n        schema_overrides={\"old_id\": pl.Int32, \"new_id\": pl.Int32},\n    )\n\n    # Join and select\n    df = df.join(\n        mapping_df, left_on=\"article_id\", right_on=\"old_id\", how=\"inner\"\n    ).select(\n        pl.col(\"new_id\").alias(\"article_id\"),\n        pl.all().exclude([\"article_id\", \"old_id\", \"new_id\"]),\n    )\n    df = df.sort(\"article_id\")\n\n    return df","metadata":{"_uuid":"521a768f-b80a-4918-83d7-39998ef112bf","_cell_guid":"8914248c-491e-483d-81c9-4d8d6b042eae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:02:02.800260Z","iopub.execute_input":"2025-04-09T08:02:02.800679Z","iopub.status.idle":"2025-04-09T08:02:02.948542Z","shell.execute_reply.started":"2025-04-09T08:02:02.800652Z","shell.execute_reply":"2025-04-09T08:02:02.947947Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#from pathlib import Path\n#from data_processing.customer_df.customer_df import get_customer_df_benchmarks\n#from data_processing.transaction_df.transaction_df import get_tx_article_dfs\nimport polars as pl\n\n\ndef expand_list_columns(\n    df: pl.DataFrame, date_col: str = \"days_before_lst\", num_col: str = \"num_items_lst\"\n) -> pl.DataFrame:\n    \"\"\"\n    Expand a Polars DataFrame by repeating each element in a list column according to\n    the counts specified in another list column.\n\n    Args:\n        df: Input Polars DataFrame with list columns\n        date_col: Name of the column containing the lists to be expanded\n        num_col: Name of the column containing lists of counts\n\n    Returns:\n        A new Polars DataFrame where the list elements in date_col have been expanded\n    \"\"\"\n    expanded = df.with_columns(\n        pl.struct([date_col, num_col])\n        .map_elements(\n            lambda x: [\n                date\n                for date, count in zip(x[date_col], x[num_col])\n                for _ in range(count)\n            ]\n        )\n        .alias(date_col)\n    )\n\n    return expanded\n\n\ndef add_benchmark_tx_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Creates benchmark transaction features from aggregated customer transaction data.\n\n    Args:\n        df: A Polars DataFrame containing aggregated transaction data with list columns\n            including total_price_lst, num_items_lst, days_before_lst, price_lst,\n            and CLV_label.\n\n    Returns:\n        pl.DataFrame: A DataFrame with derived features including:\n            - total_spent: Sum of all transaction amounts\n            - total_purchases: Count of transactions\n            - total_items: Sum of items purchased\n            - days_since_last_purchase: Days since most recent transaction\n            - days_since_first_purchase: Days since first transaction\n            - avg_spent_per_transaction: Mean transaction amount\n            - avg_items_per_transaction: Mean items per transaction\n            - avg_days_between: Mean days between transactions\n            - regression_label: CLV label for regression\n            - classification_label: Binary CLV label (>0)\n\n    Note:\n        The avg_days_between calculation may return None for customers with single\n        transactions, which is handled by tree-based algorithms.\n    \"\"\"\n    return df.select(\n        \"customer_id\",\n        pl.col(\"total_price_lst\").list.sum().alias(\"total_spent\"),\n        pl.col(\"total_price_lst\").list.len().alias(\"total_purchases\"),\n        pl.col(\"num_items_lst\").list.sum().alias(\"total_items\"),\n        pl.col(\"days_before_lst\").list.get(-1).alias(\"days_since_last_purchase\"),\n        pl.col(\"days_before_lst\").list.get(0).alias(\"days_since_first_purchase\"),\n        pl.col(\"price_lst\").list.mean().alias(\"avg_spent_per_transaction\"),\n        (\n            pl.col(\"num_items_lst\")\n            .list.mean()\n            .cast(pl.Float32)\n            .alias(\"avg_items_per_transaction\")\n        ),\n        # Code below returns None values for customers with single Tx\n        # Tree algos should be able to handle this\n        (\n            pl.col(\"days_before_lst\")\n            .list.diff(null_behavior=\"drop\")\n            .list.mean()\n            .mul(-1)\n            .cast(pl.Float32)\n            .alias(\"avg_days_between\")\n        ),\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    )\n\n\ndef process_dataframe(df: pl.DataFrame, max_length: int = 20) -> pl.DataFrame:\n    \"\"\"Processes a polars DataFrame by expanding list columns and selecting specific columns with transformations.\n\n    This function performs several operations on the input DataFrame:\n    1. Expands list columns using the expand_list_columns function\n    2. Selects and renames specific columns\n    3. Truncates list columns to a maximum length\n\n    Args:\n        df: A polars DataFrame containing customer transaction data\n        max_length: Maximum number of elements to keep in list columns (default: 20)\n\n    Returns:\n        A processed polars DataFrame with the following columns:\n            - customer_id: Customer identifier\n            - days_before_lst: Truncated list of days before some reference date\n            - articles_ids_lst: Truncated list of article identifiers\n            - regression_label: CLV label for regression tasks\n            - classification_label: Binary classification label derived from CLV\n    \"\"\"\n    df = expand_list_columns(df, date_col=\"days_before_lst\", num_col=\"num_items_lst\")\n    return df.select(\n        \"customer_id\",\n        \"days_before_lst\",\n        \"articles_ids_lst\",\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    ).with_columns(\n        pl.col(\"days_before_lst\").list.tail(max_length),\n        pl.col(\"articles_ids_lst\").list.tail(max_length),\n    )\n\n\ndef get_benchmark_dfs(\n    data_path: Path, config: dict\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Creates benchmark train, validation, and test datasets with transaction and customer features.\n\n    Args:\n        data_path: Path object pointing to the data directory\n        config: Dictionary containing configuration parameters for data processing\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: A tuple containing:\n            - train_df: Training dataset with benchmark features\n            - val_df: Validation dataset with benchmark features\n            - test_df: Test dataset with benchmark features\n\n        Each DataFrame contains transaction-derived features joined with customer features.\n    \"\"\"\n    train_article, val_article, test_article = get_tx_article_dfs(\n        data_path=data_path,\n        config=config,\n        cols_to_aggregate=[\n            \"date\",\n            \"days_before\",\n            \"article_ids\",\n            \"sales_channel_ids\",\n            \"total_price\",\n            \"prices\",\n            \"num_items\",\n        ],\n        keep_customer_id=True,\n    )\n\n    customer_df = get_customer_df_benchmarks(data_path=data_path, config=config)\n\n    train_df = process_dataframe(\n        df=train_article, max_length=config[\"max_length\"]\n    ).join(customer_df, on=\"customer_id\", how=\"left\")\n    val_df = process_dataframe(df=val_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n    test_df = process_dataframe(df=test_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n\n    return train_df, val_df, test_df","metadata":{"_uuid":"253c400e-7a25-421b-b771-120176663628","_cell_guid":"fa4e4e46-931c-496c-97fd-6154f2eb5fe3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:02:08.613177Z","iopub.execute_input":"2025-04-09T08:02:08.613465Z","iopub.status.idle":"2025-04-09T08:02:08.625709Z","shell.execute_reply.started":"2025-04-09T08:02:08.613444Z","shell.execute_reply":"2025-04-09T08:02:08.624667Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import polars as pl\n#from pathlib import Path\n\n\ndef get_customer_df_benchmarks(data_path: Path, config: dict):\n    \"\"\"Processes customer data with age grouping and zip code mapping.\n\n    Args:\n        data_path (Path): Path to directory containing 'customers.csv' and 'zip_code_count.csv'.\n        config (dict): Configuration with 'min_zip_code_count'. Updated with 'num_age_groups' and 'num_zip_codes'.\n\n    Returns:\n        pl.DataFrame: Processed DataFrame with customer_id, age_group (0-6), and mapped zip_code_id.\n    \"\"\"\n    file_path = data_path / \"customers.csv\"\n    df = pl.scan_csv(file_path).select(\n        (\n            \"customer_id\",\n            pl.col(\"age\").fill_null(strategy=\"mean\"),\n            \"postal_code\",\n        )\n    )\n\n    # df = df.with_columns(\n    #     [\n    #         pl.when(pl.col(\"age\").is_null())\n    #         .then(0)\n    #         .when(pl.col(\"age\") < 25)\n    #         .then(1)\n    #         .when(pl.col(\"age\").is_between(25, 34))\n    #         .then(2)\n    #         .when(pl.col(\"age\").is_between(35, 44))\n    #         .then(3)\n    #         .when(pl.col(\"age\").is_between(45, 54))\n    #         .then(4)\n    #         .when(pl.col(\"age\").is_between(55, 64))\n    #         .then(5)\n    #         .otherwise(6)\n    #         .alias(\"age_group\")\n    #     ]\n    # )\n    # config[\"num_age_groups\"] = 7\n\n    return df.collect()","metadata":{"_uuid":"6180c928-8750-4145-bf01-a2366dc14df0","_cell_guid":"e3f08b7c-d583-4dbf-8161-2506f9a6911f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:02:14.612743Z","iopub.execute_input":"2025-04-09T08:02:14.613051Z","iopub.status.idle":"2025-04-09T08:02:14.617482Z","shell.execute_reply.started":"2025-04-09T08:02:14.613028Z","shell.execute_reply":"2025-04-09T08:02:14.616771Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#from datetime import datetime\n#from pathlib import Path\n#import polars as pl\n\n#from data_processing.utils.utils_transaction_df import (\n #   filter_purchases_purchases_per_month_pl,\n  #  map_article_ids,\n   # train_test_split,\n#)\n\n\ndef generate_clv_data_pl(\n    df: pl.DataFrame,\n    agg_df: pl.DataFrame,\n    label_threshold: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    log_clv: bool = False,\n):\n    \"\"\"Generates Customer Lifetime Value (CLV) data from transaction dataframe.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe containing customer purchases.\n        agg_df (pl.DataFrame): Aggregated dataframe containing customer data.\n        label_threshold (datetime.date): Start date for CLV calculation period.\n        pred_end (datetime.date): End date for CLV calculation period.\n        clv_periods (list): List of periods for CLV calculation (currently supports single period only).\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated dataframe with added CLV calculations.\n\n    Raises:\n        ValueError: If more than one CLV period is provided.\n    \"\"\"\n    if len(clv_periods) > 1:\n        raise ValueError(\"CLV periods should be a single number for now.\")\n\n    # Filter transactions between label_threshold and end_date for each period\n    filtered_df = df.filter(\n        (pl.col(\"date\") >= label_threshold) & (pl.col(\"date\") <= pred_end)\n    )\n\n    # Sum total_price for the filtered transactions by customer_id. This is the CLV\n    summed_period_df = filtered_df.group_by(\"customer_id\").agg(\n        pl.sum(\"total_price\").round(2).alias(f\"CLV_label\")\n    )\n    if log_clv:\n        summed_period_df = summed_period_df.with_columns(\n            pl.col(f\"CLV_label\").log1p().round(2).alias(f\"CLV_label\")\n        )\n\n    agg_df = agg_df.join(summed_period_df, on=\"customer_id\", how=\"left\")\n\n    agg_df = agg_df.fill_null(0)\n    return agg_df\n\n\ndef group_and_convert_df_pl(\n    df: pl.DataFrame,\n    label_start_date: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"num_items\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> pl.DataFrame:\n    \"\"\"Groups and converts transaction data into aggregated customer-level features.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        label_start_date (datetime.date): Start date for clv label period.\n        pred_end (datetime.date): End date for prediction period.\n        clv_periods (list): List of periods for CLV calculation.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated customer-level dataframe.\n\n    Raises:\n        ValueError: If required columns (days_before, article_ids, num_items) are missing from cols_to_aggregate.\n    \"\"\"\n\n    if any(\n        col not in cols_to_aggregate\n        for col in [\"days_before\", \"article_ids\", \"num_items\"]\n    ):\n        raise ValueError(\n            \"The columns days_before, article_ids, and num_items are required \"\n            \"for the aggregation\"\n        )\n\n    mapping = {\n        \"date\": \"date_lst\",\n        \"days_before\": \"days_before_lst\",\n        \"article_ids\": \"articles_ids_lst\",\n        \"sales_channel_ids\": \"sales_channel_id_lst\",\n        \"total_price\": \"total_price_lst\",\n        \"prices\": \"price_lst\",\n        \"num_items\": \"num_items_lst\",\n    }\n\n    agg_df = (\n        df.filter(pl.col(\"date\") < label_start_date)\n        .with_columns(\n            (label_start_date - pl.col(\"date\"))\n            .dt.total_days()\n            .cast(pl.Int32)\n            .alias(\"days_before\"),\n            (\n                pl.col(\"sales_channel_ids\")\n                .cast(pl.List(pl.Int32))\n                .alias(\"sales_channel_ids\")\n            ),\n            pl.col(\"article_ids\").cast(pl.List(pl.Int32)).alias(\"article_ids\"),\n        )\n        .sort(\"customer_id\", \"date\")\n        .group_by(\"customer_id\")\n        .agg(\n            pl.col(\"date\").explode().alias(\"date_lst\"),\n            pl.col(\"days_before\").explode().alias(\"days_before_lst\"),\n            pl.col(\"article_ids\").explode().alias(\"articles_ids_lst\"),\n            pl.concat_list(pl.col(\"sales_channel_ids\")).alias(\"sales_channel_id_lst\"),\n            pl.col(\"total_price\").explode().alias(\"total_price_lst\"),\n            pl.col(\"prices\").explode().alias(\"price_lst\"),\n            pl.col(\"num_items\").explode().alias(\"num_items_lst\"),\n        )\n    )\n\n    if clv_periods is not None:\n        agg_df = generate_clv_data_pl(\n            df=df,\n            agg_df=agg_df,\n            label_threshold=label_start_date,\n            pred_end=pred_end,\n            clv_periods=clv_periods,\n            log_clv=log_clv,\n        )\n\n    # Drop columns which are not to be aggregated\n    cols_to_drop = [v for k, v in mapping.items() if k not in cols_to_aggregate]\n    if not keep_customer_id:\n        cols_to_drop.append(\"customer_id\")\n    agg_df = agg_df.drop(*cols_to_drop)\n\n    return agg_df\n\n\ndef split_df_and_group_pl(\n    df: pl.DataFrame,\n    clv_periods: list,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits transaction data into training and test sets and performs aggregation.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        clv_periods (list): List of periods for CLV calculation.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Aggregated training dataset\n            - test_df: Aggregated test dataset\n    \"\"\"\n\n    train_begin = datetime.strptime(config.get(\"train_begin\"), \"%Y-%m-%d\")\n    train_label_start = datetime.strptime(config.get(\"train_label_begin\"), \"%Y-%m-%d\")\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    test_begin = datetime.strptime(config.get(\"test_begin\"), \"%Y-%m-%d\")\n    test_label_start = datetime.strptime(config.get(\"test_label_begin\"), \"%Y-%m-%d\")\n    test_end = datetime.strptime(config.get(\"test_end\"), \"%Y-%m-%d\")\n\n    # Creating the training DataFrame by filtering dates up to `train_end`\n    train_df = df.filter(\n        (pl.col(\"date\") <= train_end) & (pl.col(\"date\") >= train_begin)\n    )\n\n    train_df = group_and_convert_df_pl(\n        df=train_df,\n        label_start_date=train_label_start,\n        pred_end=train_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    # Creating the test DataFrame by filtering dates after `test_begin`\n    test_df = df.filter((pl.col(\"date\") >= test_begin) & (pl.col(\"date\") <= test_end))\n\n    test_df = group_and_convert_df_pl(\n        df=test_df,\n        label_start_date=test_label_start,\n        pred_end=test_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    return train_df, test_df\n\n\ndef load_data_rem_outlier_pl(\n    data_path: Path, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Loads transaction data, applies price scaling, and removes outliers.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data parquet file.\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group data by sales channel ID. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Processed transaction dataframe\n            - extreme_customers: Dataframe of customers identified as outliers\n    \"\"\"\n    file_path = data_path / \"transactions_polars.parquet\"\n    df_pl = pl.read_parquet(file_path)\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"t_dat\").alias(\"date\").cast(pl.Date), pl.col(\"article_id\").cast(pl.Int32)\n    )\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"price\").mul(590).cast(pl.Float32).round(2).alias(\"price\")\n    )\n\n    # Map article ids to running ids so that they match with feature matrix\n    df_pl = map_article_ids(df=df_pl, data_path=data_path)\n\n    grouped_df, extreme_customers = filter_purchases_purchases_per_month_pl(\n        df_pl, train_end=train_end, group_by_channel_id=group_by_channel_id\n    )\n\n    return grouped_df, extreme_customers\n\n\ndef get_customer_train_test_articles_pl(\n    data_path: Path,\n    config: dict,\n    clv_periods: list = None,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Processes customer transaction data into train and test sets with article information.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data.\n        config (dict): Configuration dictionary for data processing parameters.\n        clv_periods (list, optional): List of periods for CLV calculation. Defaults to None.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Processed training dataset with article information\n            - test_df: Processed test dataset with article information\n    \"\"\"\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    grouped_df, extreme_customers = load_data_rem_outlier_pl(\n        data_path=data_path, train_end=train_end\n    )\n\n    train_df, test_df = split_df_and_group_pl(\n        df=grouped_df,\n        clv_periods=clv_periods,\n        config=config,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=True,\n        log_clv=config.get(\"log_clv\", False),\n    )\n\n    train_df = train_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n    test_df = test_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n\n    if not keep_customer_id:\n        train_df = train_df.drop(\"customer_id\")\n        test_df = test_df.drop(\"customer_id\")\n\n    return train_df, test_df\n\n\ndef get_tx_article_dfs(\n    data_path: Path,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Creates train, validation, and test datasets with optional subsampling.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data files.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Transaction columns to include in output.\n        keep_customer_id (bool, optional): Whether to retain customer_id column.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (subset of original training data)\n            - val_df: Validation dataset (10% of original training data)\n            - test_df: Test dataset (optionally subsampled)\n    \"\"\"\n    \"\"\"\n    Columns of dfs:\n        - customer_id\n        - date_lst (list[date]): Dates of each transaction\n        - days_before_lst (list[int]): Number of days between start of prediction and date of transction\n        - articles_ids_lst (list[int]): Flattened list of all items a customer purchased \n        - sales_channel_id_lst (list[list[int]]): Sales channel of a transaction (repeated for each item within a transaction)\n        - total_price_lst (list[float]): Value of each transaction\n        - price_lst (list[float]): Flattened list of prices of all items customer purchased\n        - num_items_lst (list[int]): Number of items in each transaction\n        - CLV_label (float): Sales in prediction period (label to be used)\n    \"\"\"\n    train_df, test_df = get_customer_train_test_articles_pl(\n        data_path=data_path,\n        config=config,\n        clv_periods=config.get(\"clv_periods\", [6]),\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n    )\n    train_df, val_df, test_df = train_test_split(\n        train_df=train_df,\n        test_df=test_df,\n        subset=config.get(\"subset\"),\n        train_subsample_percentage=config.get(\"train_subsample_percentage\"),\n    )\n    return train_df, val_df, test_df","metadata":{"_uuid":"78294ceb-43bf-4d99-9674-d35dc9939b5f","_cell_guid":"fbc039dc-8bc6-4280-a003-7e347b4fb401","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:02:26.782057Z","iopub.execute_input":"2025-04-09T08:02:26.782329Z","iopub.status.idle":"2025-04-09T08:02:26.801581Z","shell.execute_reply.started":"2025-04-09T08:02:26.782308Z","shell.execute_reply":"2025-04-09T08:02:26.800701Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#from pathlib import Path\n#from data_processing.get_data import get_benchmark_dfs\n#import polars as pl","metadata":{"_uuid":"f07fb485-6ac6-498d-bd1a-e101ef5db397","_cell_guid":"2e20626e-9d11-4933-8364-9578d162eb50","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T09:19:00.036146Z","iopub.execute_input":"2025-03-26T09:19:00.036403Z","iopub.status.idle":"2025-03-26T09:19:00.052190Z","shell.execute_reply.started":"2025-03-26T09:19:00.036375Z","shell.execute_reply":"2025-03-26T09:19:00.051420Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"config = {\n    \"train_begin\": \"2018-09-20\",\n    \"train_label_begin\": \"2019-09-20\",\n    \"train_end\": \"2020-03-17\",\n    \"test_begin\": \"2019-03-19\",\n    \"test_label_begin\": \"2020-03-18\",\n    \"test_end\": \"2020-09-13\",\n    \"min_zip_code_count\": 3,\n    \"date_aggregation\": \"daily\",\n    \"group_by_channel_id\": False,\n    \"log_clv\": False,\n    \"clv_periods\": [6],\n    \"subset\": None,\n    \"train_subsample_percentage\": None,\n    \"max_length\":20, # DEFINE HOW MANY ITEMS ARE TO BE CONSIDERED IN TRANSFORMER SEQUENCE\n}\n# data_path = Path(\"/kaggle/input/hm-dataset/data/data\")\ndata_path = Path(\"/kaggle/input/data/data/\")\n\nprint(10 * \"#\", \" Loading data \", 10 * \"#\")\ntrain_df, val_df, test_df = get_benchmark_dfs(data_path, config)","metadata":{"_uuid":"8a0c70b7-3af8-4389-8c99-aeb1a9a20ed8","_cell_guid":"efa2d661-6478-42fb-bb23-beda10fd025d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-09T08:02:30.817793Z","iopub.execute_input":"2025-04-09T08:02:30.818125Z","iopub.status.idle":"2025-04-09T08:04:16.587582Z","shell.execute_reply.started":"2025-04-09T08:02:30.818098Z","shell.execute_reply":"2025-04-09T08:04:16.586884Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"##########  Loading data  ##########\n\n        Cutoff Values for 99.0th Percentiles:\n        -----------------------------------\n        Total items bought:    152 items\n\n        -----------------------------------\n        Removed Customers:     11,908\n        \n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-8bb703141d2d>:167: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"test_df","metadata":{"_uuid":"16c73618-8eec-4473-9479-70f3a3588013","_cell_guid":"940c614a-1ed9-43c0-b246-bef5c9256f76","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-02T11:51:38.441547Z","iopub.execute_input":"2025-04-02T11:51:38.441834Z","iopub.status.idle":"2025-04-02T11:51:38.450123Z","shell.execute_reply.started":"2025-04-02T11:51:38.441812Z","shell.execute_reply":"2025-04-02T11:51:38.449342Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"shape: (951_705, 7)\n┌───────────────┬───────────────┬───────────────┬──────────────┬──────────────┬─────┬──────────────┐\n│ customer_id   ┆ days_before_l ┆ articles_ids_ ┆ regression_l ┆ classificati ┆ age ┆ postal_code  │\n│ ---           ┆ st            ┆ lst           ┆ abel         ┆ on_label     ┆ --- ┆ ---          │\n│ str           ┆ ---           ┆ ---           ┆ ---          ┆ ---          ┆ i64 ┆ str          │\n│               ┆ list[i64]     ┆ list[i32]     ┆ f32          ┆ i32          ┆     ┆              │\n╞═══════════════╪═══════════════╪═══════════════╪══════════════╪══════════════╪═════╪══════════════╡\n│ b18e866cb2b0a ┆ [58, 58, …    ┆ [70279,       ┆ 170.75       ┆ 1            ┆ 20  ┆ 9aaece90d171 │\n│ b03ed3817d713 ┆ 24]           ┆ 70279, …      ┆              ┆              ┆     ┆ eefebd53926d │\n│ 253045…       ┆               ┆ 86984]        ┆              ┆              ┆     ┆ dffe8ff0…    │\n│ c3d15502ae921 ┆ [101, 101, …  ┆ [68390,       ┆ 242.259995   ┆ 1            ┆ 51  ┆ d23d7c3a8173 │\n│ 6ea6afb65d8a7 ┆ 45]           ┆ 75638, …      ┆              ┆              ┆     ┆ c9788d12bf06 │\n│ 42bddc…       ┆               ┆ 94062]        ┆              ┆              ┆     ┆ 0060bd92…    │\n│ d20ddd1afa4f1 ┆ [276, 255, …  ┆ [16503,       ┆ 0.0          ┆ 0            ┆ 19  ┆ 0871b49aa9da │\n│ 4b79acafc8199 ┆ 255]          ┆ 33317, …      ┆              ┆              ┆     ┆ 6e9c2d31e7ab │\n│ 7bcee8…       ┆               ┆ 72731]        ┆              ┆              ┆     ┆ 916ad943…    │\n│ 3a44ab79ffe95 ┆ [305, 305, …  ┆ [13495,       ┆ 203.839996   ┆ 1            ┆ 24  ┆ 1fec1686b181 │\n│ c83f63d0416db ┆ 172]          ┆ 62773, …      ┆              ┆              ┆     ┆ f3dbe8178227 │\n│ 8c4a9b…       ┆               ┆ 77283]        ┆              ┆              ┆     ┆ 283dd761…    │\n│ 501feb5fb0bf6 ┆ [335, 335, …  ┆ [33173,       ┆ 14.99        ┆ 1            ┆ 20  ┆ 4602e46f0f84 │\n│ e873a02fb7b62 ┆ 85]           ┆ 57456, …      ┆              ┆              ┆     ┆ 0ddac8552edb │\n│ fce9bf…       ┆               ┆ 78995]        ┆              ┆              ┆     ┆ b1e2990a…    │\n│ …             ┆ …             ┆ …             ┆ …            ┆ …            ┆ …   ┆ …            │\n│ 86c457352351c ┆ [346, 329, …  ┆ [43211,       ┆ 32.970001    ┆ 1            ┆ 34  ┆ 46ac55e49e69 │\n│ 0cca5016404d9 ┆ 39]           ┆ 14863, …      ┆              ┆              ┆     ┆ 16ad78967f44 │\n│ 041a48…       ┆               ┆ 90694]        ┆              ┆              ┆     ┆ fa79a3ae…    │\n│ c95a4cc98429a ┆ [263]         ┆ [21801]       ┆ 0.0          ┆ 0            ┆ 36  ┆ 2c29ae653a92 │\n│ a89f4ddaa667e ┆               ┆               ┆              ┆              ┆     ┆ 82cce4151bd8 │\n│ 2c13fd…       ┆               ┆               ┆              ┆              ┆     ┆ 7643c907…    │\n│ ff8eaedb16b6b ┆ [110, 110, …  ┆ [2211, 69945, ┆ 0.0          ┆ 0            ┆ 21  ┆ 2c29ae653a92 │\n│ dbf1c07517886 ┆ 110]          ┆ … 78901]      ┆              ┆              ┆     ┆ 82cce4151bd8 │\n│ ae12b4…       ┆               ┆               ┆              ┆              ┆     ┆ 7643c907…    │\n│ aff1d13063853 ┆ [333, 333, …  ┆ [7861, 7862,  ┆ 0.0          ┆ 0            ┆ 22  ┆ 4b1e85b0815f │\n│ 695c336e2c47a ┆ 102]          ┆ … 67150]      ┆              ┆              ┆     ┆ 21af06c143b4 │\n│ d6c6b3…       ┆               ┆               ┆              ┆              ┆     ┆ 9932e5da…    │\n│ 62dd8248dad46 ┆ [267, 267, …  ┆ [12428,       ┆ 14.98        ┆ 1            ┆ 23  ┆ 1fe38586fd14 │\n│ 4d987d39ea5cd ┆ 129]          ┆ 43104, …      ┆              ┆              ┆     ┆ c6fa5253c233 │\n│ f16507…       ┆               ┆ 67622]        ┆              ┆              ┆     ┆ 8aa83956…    │\n└───────────────┴───────────────┴───────────────┴──────────────┴──────────────┴─────┴──────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (951_705, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>customer_id</th><th>days_before_lst</th><th>articles_ids_lst</th><th>regression_label</th><th>classification_label</th><th>age</th><th>postal_code</th></tr><tr><td>str</td><td>list[i64]</td><td>list[i32]</td><td>f32</td><td>i32</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;b18e866cb2b0ab…</td><td>[58, 58, … 24]</td><td>[70279, 70279, … 86984]</td><td>170.75</td><td>1</td><td>20</td><td>&quot;9aaece90d171ee…</td></tr><tr><td>&quot;c3d15502ae9216…</td><td>[101, 101, … 45]</td><td>[68390, 75638, … 94062]</td><td>242.259995</td><td>1</td><td>51</td><td>&quot;d23d7c3a8173c9…</td></tr><tr><td>&quot;d20ddd1afa4f14…</td><td>[276, 255, … 255]</td><td>[16503, 33317, … 72731]</td><td>0.0</td><td>0</td><td>19</td><td>&quot;0871b49aa9da6e…</td></tr><tr><td>&quot;3a44ab79ffe95c…</td><td>[305, 305, … 172]</td><td>[13495, 62773, … 77283]</td><td>203.839996</td><td>1</td><td>24</td><td>&quot;1fec1686b181f3…</td></tr><tr><td>&quot;501feb5fb0bf6e…</td><td>[335, 335, … 85]</td><td>[33173, 57456, … 78995]</td><td>14.99</td><td>1</td><td>20</td><td>&quot;4602e46f0f840d…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;86c457352351c0…</td><td>[346, 329, … 39]</td><td>[43211, 14863, … 90694]</td><td>32.970001</td><td>1</td><td>34</td><td>&quot;46ac55e49e6916…</td></tr><tr><td>&quot;c95a4cc98429aa…</td><td>[263]</td><td>[21801]</td><td>0.0</td><td>0</td><td>36</td><td>&quot;2c29ae653a9282…</td></tr><tr><td>&quot;ff8eaedb16b6bd…</td><td>[110, 110, … 110]</td><td>[2211, 69945, … 78901]</td><td>0.0</td><td>0</td><td>21</td><td>&quot;2c29ae653a9282…</td></tr><tr><td>&quot;aff1d130638536…</td><td>[333, 333, … 102]</td><td>[7861, 7862, … 67150]</td><td>0.0</td><td>0</td><td>22</td><td>&quot;4b1e85b0815f21…</td></tr><tr><td>&quot;62dd8248dad464…</td><td>[267, 267, … 129]</td><td>[12428, 43104, … 67622]</td><td>14.98</td><td>1</td><td>23</td><td>&quot;1fe38586fd14c6…</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"_uuid":"e6faeb0a-393e-4e09-9731-1abea2e34aa2","_cell_guid":"daccf64d-4dc3-43d3-8886-c1b0f337baf7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T09:20:52.723987Z","iopub.status.idle":"2025-03-26T09:20:52.724346Z","shell.execute_reply":"2025-03-26T09:20:52.724193Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.special  # for hyp2f1\nimport numpy as np\nimport pandas as pd\n\n# If you are using Polars, import it (optional)\ntry:\n    import polars as pl\nexcept ImportError:\n    pl = None\n\n##############################################################################\n# 1) BG/NBD Model (Fully Implemented: Log-Likelihood + Expected Transactions)\n##############################################################################\nclass BGNBDModel(nn.Module):\n    \"\"\"\n    BG/NBD model for transaction frequency:\n      - r, alpha : Gamma-Poisson mixture for transaction rates\n      - a, b     : Beta-Geometric mixture for dropout\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(BGNBDModel, self).__init__()\n        if init_params is None:\n            init_params = {'r': 1.0, 'alpha': 1.0, 'a': 1.0, 'b': 1.0}\n        self.log_r = nn.Parameter(torch.log(torch.tensor(init_params['r'], dtype=torch.float32)))\n        self.log_alpha = nn.Parameter(torch.log(torch.tensor(init_params['alpha'], dtype=torch.float32)))\n        self.log_a = nn.Parameter(torch.log(torch.tensor(init_params['a'], dtype=torch.float32)))\n        self.log_b = nn.Parameter(torch.log(torch.tensor(init_params['b'], dtype=torch.float32)))\n\n    def forward(self, x, t_x, T):\n        \"\"\"\n        Compute the log-likelihood (LL) for each customer under BG/NBD.\n        \"\"\"\n        # Ensure inputs are floats\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        # Recover parameters\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        ll = torch.empty_like(x)\n\n        mask0 = (x == 0)\n        mask1 = (x > 0)\n\n        # --- x = 0 case ---\n        if mask0.any():\n            T0 = T[mask0]\n            ll0 = (r * torch.log(alpha)\n                   - r * torch.log(alpha + T0)\n                   + torch.log(b)\n                   - torch.log(a + b))\n            ll[mask0] = ll0\n\n        # --- x > 0 case ---\n        if mask1.any():\n            x1   = x[mask1]\n            t_x1 = t_x[mask1]\n            T1   = T[mask1]\n\n            term1 = ( torch.lgamma(r + x1)\n                      - torch.lgamma(r)\n                      - torch.lgamma(x1 + 1) )\n            term2 = r * ( torch.log(alpha) - torch.log(alpha + T1) )\n            term3 = x1 * ( torch.log(T1 - t_x1) - torch.log(alpha + T1) )\n            term4 = ( torch.log(a)\n                      + torch.lgamma(a + b)\n                      - torch.lgamma(a)\n                      - torch.lgamma(a + b + x1)\n                      + torch.lgamma(a + x1) )\n            z = (T1 - t_x1) / (alpha + T1)\n            # hypergeometric term\n            hyp_val = torch.special.hyp2f1(r + x1, a, a + b + x1, z)\n            term5 = torch.log(hyp_val + 1e-30)\n\n            ll1 = term1 + term2 + term3 + term4 + term5\n            ll[mask1] = ll1\n\n        return ll\n\n    def negative_log_likelihood(self, x, t_x, T):\n        ll = self.forward(x, t_x, T)\n        return -ll.sum()\n\n    def expected_transactions(self, x, t_x, T, t_future=10.0):\n        \"\"\"\n        Approximate E[# of transactions in (T, T+t_future)].\n        We'll use a simplified approximation:\n            E[X(t_future)|...] ≈ p_alive * ((r + x)/(alpha + T)) * t_future\n        where p_alive is computed using a formula from Hardie's notes.\n        \"\"\"\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        # Compute log_factor for p_alive\n        log_factor = ( torch.lgamma(a + 1)\n                       + torch.lgamma(b + x)\n                       - torch.lgamma(a)\n                       - torch.lgamma(b + x + 1) )\n        log_factor += (r + x) * ( torch.log(alpha + T) - torch.log(alpha + t_x + 1e-8) )\n        factor = torch.exp(log_factor)\n        p_alive = 1.0 / (1.0 + factor)\n\n        return p_alive * ((r + x) / (alpha + T + 1e-8)) * t_future\n\n##############################################################################\n# 2) Gamma–Gamma Model (Fully Implemented)\n##############################################################################\nclass GammaGammaModel(nn.Module):\n    \"\"\"\n    Gamma–Gamma model for monetary value.\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(GammaGammaModel, self).__init__()\n        if init_params is None:\n            init_params = {'p': 1.0, 'q': 1.0, 'v': 1.0}\n        self.log_p = nn.Parameter(torch.log(torch.tensor(init_params['p'], dtype=torch.float32)))\n        self.log_q = nn.Parameter(torch.log(torch.tensor(init_params['q'], dtype=torch.float32)))\n        self.log_v = nn.Parameter(torch.log(torch.tensor(init_params['v'], dtype=torch.float32)))\n\n    def forward(self, x, m):\n        \"\"\"\n        Compute the log-likelihood for the Gamma–Gamma model.\n        \"\"\"\n        x = x.float()\n        m = m.float()\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n        eps = 1e-30\n        term1 = torch.lgamma(p + q*x) - torch.lgamma(p) - torch.lgamma(q*x + eps)\n        term2 = p * torch.log(v + eps)\n        term3 = (q - 1) * x * torch.log(m + eps)\n        term4 = - (p + q*x) * torch.log(v + x*m + eps)\n        return term1 + term2 + term3 + term4\n\n    def negative_log_likelihood(self, x, m):\n        ll = self.forward(x, m)\n        return -ll.sum()\n\n    def conditional_expected_value(self, x, m):\n        \"\"\"\n        Compute E[m|x, m] = (p + q*x) / (v + x*m).\n        \"\"\"\n        x = x.float()\n        m = m.float()\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n        eps = 1e-8\n        return (p + q * x) / (v + x*m + eps)\n\n##############################################################################\n# 3) Composite Model: Multiply BG/NBD * Gamma–Gamma for CLV Prediction\n##############################################################################\nclass CompositeCLVModel(nn.Module):\n    \"\"\"\n    Composite model that predicts future CLV as:\n         Predicted CLV = E[# future transactions] * E[monetary value]\n    \"\"\"\n    def __init__(self, init_bgnbd=None, init_gg=None, t_future=10.0):\n        super(CompositeCLVModel, self).__init__()\n        self.bgnbd = BGNBDModel(init_params=init_bgnbd)\n        self.ggamma = GammaGammaModel(init_params=init_gg)\n        self.t_future = t_future\n\n    def forward(self, x, t_x, T, m):\n        count_pred = self.bgnbd.expected_transactions(x, t_x, T, self.t_future)\n        value_pred = self.ggamma.conditional_expected_value(x, m)\n        return count_pred * value_pred\n\n    def loss_mse(self, x, t_x, T, m, actual_spend):\n        pred = self.forward(x, t_x, T, m)\n        return torch.mean((pred - actual_spend)**2)\n\n##############################################################################\n# 4) Data Preparation: Convert DataFrame to Tensors\n##############################################################################\ndef parse_x_t_x_T(row):\n    \"\"\"\n    Parse (x, t_x, T) from 'days_before_lst'.\n    \"\"\"\n    days_list = row['days_before_lst']\n    if not isinstance(days_list, list) or len(days_list) == 0:\n        return 0, 0.0, 0.0\n    else:\n        x = len(days_list)\n        T = float(sum(days_list))\n        t_x = float(days_list[-1])\n        return x, t_x, T\n\ndef parse_avg_monetary_value(row):\n    \"\"\"\n    Parse a dummy average monetary value from 'articles_ids_lst'.\n    Here we define: m = 20 + 0.1 * (number of articles).\n    \"\"\"\n    arts = row['articles_ids_lst']\n    if not isinstance(arts, list) or len(arts) == 0:\n        return 20.0\n    else:\n        return 20.0 + 0.1 * len(arts)\n\ndef build_tensor_dataset(df):\n    \"\"\"\n    Build PyTorch tensors from the DataFrame.\n    Expects columns:\n      - 'days_before_lst'\n      - 'articles_ids_lst'\n      - 'regression_label'\n    \"\"\"\n    # If the DataFrame is a Polars DataFrame, convert it to Pandas.\n    if not hasattr(df, \"iterrows\"):\n        df = df.to_pandas()\n        \n    x_list, t_x_list, T_list, m_list, spend_list = [], [], [], [], []\n    for _, row in df.iterrows():\n        x_val, tx_val, T_val = parse_x_t_x_T(row)\n        m_val = parse_avg_monetary_value(row)\n        clv = row.get('regression_label', 0.0)\n        x_list.append(x_val)\n        t_x_list.append(tx_val)\n        T_list.append(T_val)\n        m_list.append(m_val)\n        spend_list.append(clv)\n    x_ten = torch.tensor(x_list, dtype=torch.float32)\n    t_x_ten = torch.tensor(t_x_list, dtype=torch.float32)\n    T_ten = torch.tensor(T_list, dtype=torch.float32)\n    m_ten = torch.tensor(m_list, dtype=torch.float32)\n    spend_ten = torch.tensor(spend_list, dtype=torch.float32)\n    return x_ten, t_x_ten, T_ten, m_ten, spend_ten\n\n##############################################################################\n# 5) Full Pipeline: Train and Validate the Composite Model\n##############################################################################\ndef train_and_validate(train_df, val_df, test_df):\n    # Build tensors from DataFrames\n    x_train, t_x_train, T_train, m_train, spend_train = build_tensor_dataset(train_df)\n    x_val, t_x_val, T_val, m_val, spend_val = build_tensor_dataset(val_df)\n    x_test, t_x_test, T_test, m_test, spend_test = build_tensor_dataset(test_df)\n\n    # Create composite model\n    model = CompositeCLVModel(\n        init_bgnbd={'r':1.0, 'alpha':1.0, 'a':1.0, 'b':1.0},\n        init_gg={'p':1.0, 'q':1.0, 'v':1.0},\n        t_future=10.0\n    )\n    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n    n_epochs = 500\n\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        loss = model.loss_mse(x_train, t_x_train, T_train, m_train, spend_train)\n        loss.backward()\n        optimizer.step()\n        if (epoch+1) % 100 == 0:\n            with torch.no_grad():\n                val_preds = model.forward(x_val, t_x_val, T_val, m_val)\n                val_loss = torch.mean((val_preds - spend_val)**2)\n            print(f\"Epoch {epoch+1}/{n_epochs} | Train MSE: {loss.item():.4f} | Val MSE: {val_loss.item():.4f}\")\n\n    with torch.no_grad():\n        test_preds = model.forward(x_test, t_x_test, T_test, m_test)\n        test_mse = torch.mean((test_preds - spend_test)**2).item()\n    print(f\"\\nFinal Test MSE: {test_mse:.4f}\")\n\n    # Print learned parameters\n    print(\"\\nLearned BG/NBD parameters:\")\n    print(\"r     =\", torch.exp(model.bgnbd.log_r).item())\n    print(\"alpha =\", torch.exp(model.bgnbd.log_alpha).item())\n    print(\"a     =\", torch.exp(model.bgnbd.log_a).item())\n    print(\"b     =\", torch.exp(model.bgnbd.log_b).item())\n\n    print(\"\\nLearned Gamma–Gamma parameters:\")\n    print(\"p =\", torch.exp(model.ggamma.log_p).item())\n    print(\"q =\", torch.exp(model.ggamma.log_q).item())\n    print(\"v =\", torch.exp(model.ggamma.log_v).item())\n\n##############################################################################\n# 6) DEMO: Using Dummy Data (Replace with your real train_df, val_df, test_df)\n##############################################################################\ntrain_and_validate(train_df, val_df, test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:07:38.799927Z","iopub.execute_input":"2025-04-02T12:07:38.800299Z","iopub.status.idle":"2025-04-02T12:09:48.893700Z","shell.execute_reply.started":"2025-04-02T12:07:38.800269Z","shell.execute_reply":"2025-04-02T12:09:48.892666Z"}},"outputs":[{"name":"stdout","text":"Epoch 100/500 | Train MSE: 27973.7207 | Val MSE: 28223.3125\nEpoch 200/500 | Train MSE: 27968.4219 | Val MSE: 28217.3984\nEpoch 300/500 | Train MSE: 27968.4219 | Val MSE: 28217.3945\nEpoch 400/500 | Train MSE: 27968.4219 | Val MSE: 28217.3926\nEpoch 500/500 | Train MSE: 27968.4219 | Val MSE: 28217.3926\n\nFinal Test MSE: 35264.6523\n\nLearned BG/NBD parameters:\nr     = 1.8511732816696167\nalpha = 0.5401979684829712\na     = 0.5455837845802307\nb     = 1.8328989744186401\n\nLearned Gamma–Gamma parameters:\np = 1.8511734008789062\nq = 1.0\nv = 0.5401979088783264\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install pymc_marketing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T08:04:51.459797Z","iopub.execute_input":"2025-04-09T08:04:51.460105Z","iopub.status.idle":"2025-04-09T08:05:02.569702Z","shell.execute_reply.started":"2025-04-09T08:04:51.460082Z","shell.execute_reply":"2025-04-09T08:05:02.568675Z"}},"outputs":[{"name":"stdout","text":"Collecting pymc_marketing\n  Downloading pymc_marketing-0.12.1-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: arviz>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (0.20.0)\nRequirement already satisfied: matplotlib>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (3.9.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (2.2.3)\nCollecting preliz>=0.8.0 (from pymc_marketing)\n  Downloading preliz-0.11.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pydantic>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (2.11.0a2)\nCollecting pymc>=5.21.1 (from pymc_marketing)\n  Downloading pymc-5.22.0-py3-none-any.whl.metadata (16 kB)\nCollecting pyprojroot (from pymc_marketing)\n  Downloading pyprojroot-0.3.0-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: scikit-learn>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (1.5.2)\nRequirement already satisfied: seaborn>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (0.13.2)\nRequirement already satisfied: xarray-einstats>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (0.8.0)\nRequirement already satisfied: xarray>=2024.1.0 in /usr/local/lib/python3.10/dist-packages (from pymc_marketing) (2024.11.0)\nRequirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.10/dist-packages (from arviz>=0.13.0->pymc_marketing) (75.1.0)\nRequirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from arviz>=0.13.0->pymc_marketing) (1.14.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from arviz>=0.13.0->pymc_marketing) (24.2)\nRequirement already satisfied: h5netcdf>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from arviz>=0.13.0->pymc_marketing) (1.4.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from arviz>=0.13.0->pymc_marketing) (4.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (4.55.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (1.4.7)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->pymc_marketing) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pymc_marketing) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pymc_marketing) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->pymc_marketing) (2024.2)\nRequirement already satisfied: numba>=0.59 in /usr/local/lib/python3.10/dist-packages (from preliz>=0.8.0->pymc_marketing) (0.60.0)\nCollecting scipy>=1.9.0 (from arviz>=0.13.0->pymc_marketing)\n  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.1.0->pymc_marketing) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.1.0->pymc_marketing) (2.29.0)\nRequirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from pymc>=5.21.1->pymc_marketing) (5.5.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from pymc>=5.21.1->pymc_marketing) (3.1.0)\nCollecting pytensor<2.31,>=2.30.2 (from pymc>=5.21.1->pymc_marketing)\n  Downloading pytensor-2.30.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10.0 kB)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from pymc>=5.21.1->pymc_marketing) (13.9.4)\nRequirement already satisfied: threadpoolctl<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from pymc>=5.21.1->pymc_marketing) (3.5.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.1->pymc_marketing) (1.4.2)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from h5netcdf>=1.0.2->arviz>=0.13.0->pymc_marketing) (3.12.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.59->preliz>=0.8.0->pymc_marketing) (0.43.0)\nRequirement already satisfied: filelock>=3.15 in /usr/local/lib/python3.10/dist-packages (from pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (3.16.1)\nRequirement already satisfied: etuples in /usr/local/lib/python3.10/dist-packages (from pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (0.3.9)\nRequirement already satisfied: logical-unification in /usr/local/lib/python3.10/dist-packages (from pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (0.4.6)\nRequirement already satisfied: miniKanren in /usr/local/lib/python3.10/dist-packages (from pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (1.0.3)\nRequirement already satisfied: cons in /usr/local/lib/python3.10/dist-packages (from pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (0.4.6)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.1->pymc_marketing) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->pymc>=5.21.1->pymc_marketing) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->pymc>=5.21.1->pymc_marketing) (2.18.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->pymc_marketing) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->pymc_marketing) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->pymc_marketing) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->pymc_marketing) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->pymc_marketing) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->pymc>=5.21.1->pymc_marketing) (0.1.2)\nRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from logical-unification->pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (0.12.1)\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from logical-unification->pytensor<2.31,>=2.30.2->pymc>=5.21.1->pymc_marketing) (1.0.0)\nDownloading pymc_marketing-0.12.1-py3-none-any.whl (253 kB)\nDownloading preliz-0.11.0-py3-none-any.whl (514 kB)\nDownloading pymc-5.22.0-py3-none-any.whl (517 kB)\nDownloading pyprojroot-0.3.0-py3-none-any.whl (7.6 kB)\nDownloading pytensor-2.30.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyprojroot, scipy, pytensor, pymc, preliz, pymc_marketing\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n  Attempting uninstall: pytensor\n    Found existing installation: pytensor 2.26.4\n    Uninstalling pytensor-2.26.4:\n      Successfully uninstalled pytensor-2.26.4\n  Attempting uninstall: pymc\n    Found existing installation: pymc 5.19.1\n    Uninstalling pymc-5.19.1:\n      Successfully uninstalled pymc-5.19.1\nSuccessfully installed preliz-0.11.0 pymc-5.22.0 pymc_marketing-0.12.1 pyprojroot-0.3.0 pytensor-2.30.2 scipy-1.12.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport pymc_marketing.clv as clv\nfrom pymc_marketing.prior import Prior\n\n\ndef aggregate_transactions_polars(\n    df: pl.DataFrame,\n    customer_id_col: str = 'customer_id',\n    date_col: str = 'days_before_lst',\n    monetary_col: str = 'regression_label'\n) -> pl.DataFrame:\n    # Create a struct column so that the parallel lists can be exploded together.\n    df = df.with_columns(\n        pl.struct([pl.col(date_col), pl.col(monetary_col)]).alias(\"transaction\")\n    )\n    # Explode the transaction column so that each element becomes its own row.\n    df_exploded = df.explode(\"transaction\")\n    # Extract the fields from the struct into separate columns.\n    df_exploded = df_exploded.with_columns([\n        pl.col(\"transaction\").struct.field(date_col).alias(\"transaction_day\"),\n        pl.col(\"transaction\").struct.field(monetary_col).alias(\"transaction_value\")\n    ])\n    # Group by the customer id and compute the aggregates.\n    aggregated = (\n        df_exploded.groupby(customer_id_col)\n                   .agg([\n                       pl.col(\"transaction_day\").min().alias(\"first_purchase\"),\n                       pl.col(\"transaction_day\").max().alias(\"last_purchase\"),\n                       pl.count(\"transaction_day\").alias(\"transaction_count\"),\n                       pl.col(\"transaction_value\").sum().alias(\"total_value\")\n                   ])\n    )\n    # Compute derived metrics:\n    aggregated = aggregated.with_columns([\n        (pl.col(\"transaction_count\") - 1).alias(\"frequency\"),\n        (pl.col(\"last_purchase\") - pl.col(\"first_purchase\")).alias(\"recency\"),\n        # Define T as the span of observed time (adjust this as needed)\n        (pl.col(\"last_purchase\") - pl.col(\"first_purchase\")).alias(\"T\"),\n        (pl.col(\"total_value\") / pl.col(\"transaction_count\")).alias(\"monetary_value\")\n    ])\n    # Select the required columns.\n    aggregated = aggregated.select([customer_id_col, \"frequency\", \"recency\", \"T\", \"monetary_value\"])\n    return aggregated\n\n\ntrain_summary_polars = aggregate_transactions_polars(train_df)\nval_summary_polars   = aggregate_transactions_polars(val_df)\ntest_summary_polars  = aggregate_transactions_polars(test_df)\n\n\ntrain_summary = train_summary_polars.to_pandas()\nval_summary   = val_summary_polars.to_pandas()\ntest_summary  = test_summary_polars.to_pandas()\n\n\nprint(\"Aggregated Training Summary:\")\nprint(train_summary.head())\n\n# The BetaGeoModel requires the aggregated DataFrame (with columns 'frequency',\n# 'recency', 'T', and 'customer_id') as its first positional argument.\n# We also optionally pass a model configuration for the priors as well as a sampler configuration.\n\n# Define a prior configuration for illustration.\nprior_distribution = {\"dist\": \"Gamma\", \"kwargs\": {\"alpha\": 0.1, \"beta\": 0.1}}\nmodel_config = {\n    \"r_prior\": prior_distribution,\n    \"alpha_prior\": prior_distribution,\n    \"a_prior\": prior_distribution,\n    \"b_prior\": prior_distribution,\n}\nsampler_config = {\n    \"draws\": 500,   # Fewer draws for demonstration (increase for production)\n    \"tune\": 500,\n    \"chains\": 2,\n    \"cores\": 2\n}\n\n# Instantiate, build, and fit the BetaGeoModel on the training summary.\nbeta_geo_model = clv.BetaGeoModel(train_summary,\n                                  model_config=model_config,\n                                  sampler_config=sampler_config)\nbeta_geo_model.build_model()\nbeta_geo_model.fit()\nprint(\"\\nBetaGeoModel Fit Summary:\")\nprint(beta_geo_model.fit_summary())\n\n\n# Fit the Monetary Model: GammaGammaFitter\n\n# The GammaGammaFitter models the monetary value per transaction.\n# It requires the training summary with the 'monetary_value' and 'frequency' columns.\ngamma_gamma_model = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_gamma_model.build_model()\ngamma_gamma_model.fit()\nprint(\"\\nGammaGammaFitter Fit Summary:\")\nprint(gamma_gamma_model.fit_summary())\n\n\n# Combine the Models to Predict CLV\n\n# The GammaGammaFitter provides a method 'predict_clv' that combines the estimates from\n# the transaction model (BetaGeoModel) and the monetary model.\n# We now predict the customer lifetime value (CLV) over a defined future time horizon\n# on both validation and test summaries.\n\n# Set prediction parameters:\ntime_horizon = 30    # e.g. future time period (in days)\ndiscount_rate = 0.01 # example discount rate\n\n# Predict CLV on the validation summary.\nval_pred_clv = gamma_gamma_model.predict_clv(val_summary,\n                                             transaction_model=beta_geo_model,\n                                             time=time_horizon,\n                                             discount_rate=discount_rate)\n\nval_rmse = np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv))\nprint(\"\\nValidation CLV RMSE: {:.4f}\".format(val_rmse))\n\n# Predict CLV on the test summary.\ntest_pred_clv = gamma_gamma_model.predict_clv(test_summary,\n                                              transaction_model=beta_geo_model,\n                                              time=time_horizon,\n                                              discount_rate=discount_rate)\ntest_rmse = np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv))\nprint(\"Test CLV RMSE: {:.4f}\".format(test_rmse))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport pymc_marketing.clv as clv\nfrom pymc_marketing.prior import Prior\n\ndef aggregate_customer_summary(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns([\n        pl.col(\"days_before_lst\").arr.eval(pl.element().cast(pl.Float64)).alias(\"days_before_lst_f64\"),\n        pl.col(\"regression_label\").arr.eval(pl.element().cast(pl.Float64)).alias(\"regression_label_f64\")\n    ])\n    aggregated = df.select([\n        pl.col(\"days_before_lst_f64\").arr.min().alias(\"first_purchase\"),\n        pl.col(\"days_before_lst_f64\").arr.max().alias(\"last_purchase\"),\n        (pl.col(\"days_before_lst_f64\").arr.lengths() - 1).alias(\"frequency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"recency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"T\"),\n        (pl.col(\"regression_label_f64\").arr.sum() / pl.col(\"regression_label_f64\").arr.lengths()).alias(\"monetary_value\")\n    ])\n    return aggregated\n\ntrain_summary_polars = aggregate_customer_summary(train_df)\nval_summary_polars = aggregate_customer_summary(val_df)\ntest_summary_polars = aggregate_customer_summary(test_df)\ntrain_summary = train_summary_polars.to_pandas()\nval_summary = val_summary_polars.to_pandas()\ntest_summary = test_summary_polars.to_pandas()\n\nprior_distribution = {\"dist\": \"Gamma\", \"kwargs\": {\"alpha\": 0.1, \"beta\": 0.1}}\nmodel_config = {\"r_prior\": prior_distribution, \"alpha_prior\": prior_distribution, \"a_prior\": prior_distribution, \"b_prior\": prior_distribution}\nsampler_config = {\"draws\": 500, \"tune\": 500, \"chains\": 2, \"cores\": 2}\n\nbeta_geo_model = clv.BetaGeoModel(train_summary, model_config=model_config, sampler_config=sampler_config)\nbeta_geo_model.build_model()\nbeta_geo_model.fit()\ngamma_gamma_model = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_gamma_model.build_model()\ngamma_gamma_model.fit()\ntime_horizon = 30\ndiscount_rate = 0.01\nval_pred_clv = gamma_gamma_model.predict_clv(val_summary, transaction_model=beta_geo_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv = gamma_gamma_model.predict_clv(test_summary, transaction_model=beta_geo_model, time=time_horizon, discount_rate=discount_rate)\nval_rmse = np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv))\ntest_rmse = np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv))\nprint(val_rmse)\nprint(test_rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport pymc_marketing.clv as clv\nfrom pymc_marketing.prior import Prior\n\ndef aggregate_customer_summary(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns([\n        pl.col(\"days_before_lst\").arr.eval(pl.element().cast(pl.Float64)).alias(\"days_before_lst_f64\"),\n        pl.col(\"regression_label\").arr.eval(pl.element().cast(pl.Float64)).alias(\"regression_label_f64\")\n    ])\n    aggregated = df.select([\n        pl.col(\"days_before_lst_f64\").arr.min().alias(\"first_purchase\"),\n        pl.col(\"days_before_lst_f64\").arr.max().alias(\"last_purchase\"),\n        (pl.col(\"days_before_lst_f64\").arr.lengths() - 1).alias(\"frequency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"recency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"T\"),\n        (pl.col(\"regression_label_f64\").arr.sum() / pl.col(\"regression_label_f64\").arr.lengths()).alias(\"monetary_value\")\n    ])\n    return aggregated\n\ntrain_summary_polars = aggregate_customer_summary(train_df)\nval_summary_polars = aggregate_customer_summary(val_df)\ntest_summary_polars = aggregate_customer_summary(test_df)\ntrain_summary = train_summary_polars.to_pandas()\nval_summary = val_summary_polars.to_pandas()\ntest_summary = test_summary_polars.to_pandas()\n\nsampler_config = {\"draws\": 500, \"tune\": 500, \"chains\": 2, \"cores\": 2}\n\npareto_model = clv.ParetoNBDFitter(train_summary, sampler_config=sampler_config)\npareto_model.build_model()\npareto_model.fit()\ngamma_gamma_model = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_gamma_model.build_model()\ngamma_gamma_model.fit()\ntime_horizon = 30\ndiscount_rate = 0.01\nval_pred_clv = gamma_gamma_model.predict_clv(val_summary, transaction_model=pareto_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv = gamma_gamma_model.predict_clv(test_summary, transaction_model=pareto_model, time=time_horizon, discount_rate=discount_rate)\nval_rmse = np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv))\ntest_rmse = np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv))\nprint(val_rmse)\nprint(test_rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport pymc_marketing.clv as clv\nfrom pymc_marketing.prior import Prior\n\ndef aggregate_customer_summary(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns([\n        pl.col(\"days_before_lst\").arr.eval(pl.element().cast(pl.Float64)).alias(\"days_before_lst_f64\"),\n        pl.col(\"regression_label\").arr.eval(pl.element().cast(pl.Float64)).alias(\"regression_label_f64\")\n    ])\n    aggregated = df.select([\n        pl.col(\"days_before_lst_f64\").arr.min().alias(\"first_purchase\"),\n        pl.col(\"days_before_lst_f64\").arr.max().alias(\"last_purchase\"),\n        (pl.col(\"days_before_lst_f64\").arr.lengths() - 1).alias(\"frequency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"recency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"T\"),\n        (pl.col(\"regression_label_f64\").arr.sum() / pl.col(\"regression_label_f64\").arr.lengths()).alias(\"monetary_value\")\n    ])\n    return aggregated\n\ntrain_summary_polars = aggregate_customer_summary(train_df)\nval_summary_polars = aggregate_customer_summary(val_df)\ntest_summary_polars = aggregate_customer_summary(test_df)\ntrain_summary = train_summary_polars.to_pandas()\nval_summary = val_summary_polars.to_pandas()\ntest_summary = test_summary_polars.to_pandas()\n\nsampler_config = {\"draws\": 500, \"tune\": 500, \"chains\": 2, \"cores\": 2}\n\nmodified_model = clv.ModifiedBetaGeoModel(train_summary, sampler_config=sampler_config)\nmodified_model.build_model()\nmodified_model.fit()\ngamma_gamma_model = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_gamma_model.build_model()\ngamma_gamma_model.fit()\ntime_horizon = 30\ndiscount_rate = 0.01\nval_pred_clv = gamma_gamma_model.predict_clv(val_summary, transaction_model=modified_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv = gamma_gamma_model.predict_clv(test_summary, transaction_model=modified_model, time=time_horizon, discount_rate=discount_rate)\nval_rmse = np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv))\ntest_rmse = np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv))\nprint(val_rmse)\nprint(test_rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# All 3 pipelines\n\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport pymc_marketing.clv as clv\nfrom pymc_marketing.prior import Prior\n\n# Aggregate raw customer data (without customer_id as feature)\ndef aggregate_customer_summary(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.with_columns([\n        pl.col(\"days_before_lst\").arr.eval(pl.element().cast(pl.Float64)).alias(\"days_before_lst_f64\"),\n        pl.col(\"regression_label\").arr.eval(pl.element().cast(pl.Float64)).alias(\"regression_label_f64\")\n    ])\n    aggregated = df.select([\n        pl.col(\"days_before_lst_f64\").arr.min().alias(\"first_purchase\"),\n        pl.col(\"days_before_lst_f64\").arr.max().alias(\"last_purchase\"),\n        (pl.col(\"days_before_lst_f64\").arr.lengths() - 1).alias(\"frequency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"recency\"),\n        (pl.col(\"days_before_lst_f64\").arr.max() - pl.col(\"days_before_lst_f64\").arr.min()).alias(\"T\"),\n        (pl.col(\"regression_label_f64\").arr.sum() / pl.col(\"regression_label_f64\").arr.lengths()).alias(\"monetary_value\")\n    ])\n    return aggregated\n\n\n# Aggregate and convert to Pandas\ntrain_summary = aggregate_customer_summary(train_df).to_pandas()\nval_summary   = aggregate_customer_summary(val_df).to_pandas()\ntest_summary  = aggregate_customer_summary(test_df).to_pandas()\n\nsampler_config = {\"draws\": 500, \"tune\": 500, \"chains\": 2, \"cores\": 2}\nprior_distribution = {\"dist\": \"Gamma\", \"kwargs\": {\"alpha\": 0.1, \"beta\": 0.1}}\nmodel_config = {\"r_prior\": prior_distribution, \"alpha_prior\": prior_distribution,\n                \"a_prior\": prior_distribution, \"b_prior\": prior_distribution}\ntime_horizon = 30\ndiscount_rate = 0.01\n\n\n# 1: BG/NBD (BetaGeoModel) + Gamma–Gamma\n\nprint(\"=== BG/NBD (BetaGeoModel) + GammaGamma ===\")\nbeta_geo_model = clv.BetaGeoModel(train_summary, model_config=model_config, sampler_config=sampler_config)\nbeta_geo_model.build_model()\nbeta_geo_model.fit()\ngamma_geo_model1 = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_geo_model1.build_model()\ngamma_geo_model1.fit()\nval_pred_clv1 = gamma_geo_model1.predict_clv(val_summary, transaction_model=beta_geo_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv1 = gamma_geo_model1.predict_clv(test_summary, transaction_model=beta_geo_model, time=time_horizon, discount_rate=discount_rate)\nprint(\"BetaGeoModel Validation RMSE:\", np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv1)))\nprint(\"BetaGeoModel Test RMSE:\", np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv1)))\nprint(\"====================================================\\n\")\n\n\n# 2: Pareto/NBD (ParetoNBDFitter) + Gamma–Gamma\n\nprint(\"=== Pareto/NBD (ParetoNBDFitter) + GammaGamma ===\")\npareto_model = clv.ParetoNBDFitter(train_summary, sampler_config=sampler_config)\npareto_model.build_model()\npareto_model.fit()\ngamma_geo_model2 = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_geo_model2.build_model()\ngamma_geo_model2.fit()\nval_pred_clv2 = gamma_geo_model2.predict_clv(val_summary, transaction_model=pareto_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv2 = gamma_geo_model2.predict_clv(test_summary, transaction_model=pareto_model, time=time_horizon, discount_rate=discount_rate)\nprint(\"ParetoNBDFitter Validation RMSE:\", np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv2)))\nprint(\"ParetoNBDFitter Test RMSE:\", np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv2)))\nprint(\"====================================================\\n\")\n\n\n# 3: Modified BG/NBD (ModifiedBetaGeoModel) + Gamma–Gamma\n\nprint(\"=== Modified BG/NBD (ModifiedBetaGeoModel) + GammaGamma ===\")\nmodified_model = clv.ModifiedBetaGeoModel(train_summary, sampler_config=sampler_config)\nmodified_model.build_model()\nmodified_model.fit()\ngamma_geo_model3 = clv.GammaGammaFitter(train_summary, sampler_config=sampler_config)\ngamma_geo_model3.build_model()\ngamma_geo_model3.fit()\nval_pred_clv3 = gamma_geo_model3.predict_clv(val_summary, transaction_model=modified_model, time=time_horizon, discount_rate=discount_rate)\ntest_pred_clv3 = gamma_geo_model3.predict_clv(test_summary, transaction_model=modified_model, time=time_horizon, discount_rate=discount_rate)\nprint(\"ModifiedBetaGeoModel Validation RMSE:\", np.sqrt(mean_squared_error(val_summary[\"monetary_value\"], val_pred_clv3)))\nprint(\"ModifiedBetaGeoModel Test RMSE:\", np.sqrt(mean_squared_error(test_summary[\"monetary_value\"], test_pred_clv3)))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}