{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -r /kaggle/input/requirements1.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n\nclass StatelessRandomGenerator:\n    def __init__(self, seed=42):\n        self.seed = seed\n\n    def set_seed(self, new_seed):\n        self.seed = new_seed\n\n    def random(self, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.random(size)\n\n    def integers(self, low, high=None, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.integers(low, high, size)\n\n    def choice(self, a, size=None, replace=True, p=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.choice(a, size, replace, p)\n\n\nglobal_rng = StatelessRandomGenerator(42)\n\n\ndef set_global_seed(new_seed):\n    global_rng.set_seed(new_seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\ndef wmape_metric(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n    return torch.sum(torch.abs(pred - true), dim=0) / torch.sum(true, dim=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\nimport json\nfrom pathlib import Path\nimport polars as pl\n#from data_processing.utils.stateless_rng import global_rng\n\ndef filter_purchases_purchases_per_month_pl(\n    df_pl: pl.DataFrame, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Filters extreme customers and groups purchases by date and optionally by sales channel.\n\n    This function:\n    1. Groups transactions by customer, date, and optionally sales channel\n    2. Identifies extreme customers based on the 99th percentile of total items purchased\n    3. Removes these customers from the dataset\n\n    Args:\n        df_pl (pl.DataFrame): Input transaction dataframe containing:\n            - customer_id: Customer identifier\n            - date: Transaction date\n            - article_id: Product identifier\n            - price: Transaction price\n            - sales_channel_id: Sales channel identifier\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group transactions by sales channel. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Grouped transaction data with columns:\n                - customer_id, date, [sales_channel_id], article_ids, total_price, prices, num_items\n            - extreme_customers: DataFrame of customers identified as outliers based on purchase behavior\n\n    Notes:\n        Extreme customers are identified using the 99th percentile of total items purchased\n        during the training period.\n    \"\"\"\n    # Used for multi variate time series\n    if group_by_channel_id:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\", \"sales_channel_id\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n    else:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"sales_channel_id\").explode().alias(\"sales_channel_ids\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n\n    # Only remove customers with extreme purchases in train period\n    customers_summary = (\n        df_pl.lazy()\n        .filter(pl.col(\"date\") < train_end)\n        .group_by(\"customer_id\")\n        .agg(\n            [\n                pl.col(\"date\").n_unique().alias(\"total_purchases\"),\n                pl.col(\"price\").sum().round(2).alias(\"total_spent\"),\n                pl.col(\"article_id\").flatten().alias(\"flattened_ids\")\n            ]\n        )\n        .with_columns(pl.col(\"flattened_ids\").list.len().alias(\"total_items\"))\n    )\n\n    quantile = 0.99\n    total_purchases_99, total_spending_99, total_items_99 = (\n        customers_summary.select(\n            [\n                pl.col(\"total_purchases\").quantile(quantile),\n                pl.col(\"total_spent\").quantile(quantile),\n                pl.col(\"total_items\").quantile(quantile),\n            ]\n        )\n        .collect()\n        .to_numpy()\n        .flatten()\n    )\n\n    # Currently only remove customers with very large number of total items purchased\n    extreme_customers = customers_summary.filter(\n        (pl.col(\"total_items\") >= total_items_99)\n        # | (pl.col(\"total_purchases\") >= total_purchases_99)\n        # | (pl.col(\"total_spent\") >= total_spending_99)\n    )\n\n    extreme_customers = extreme_customers.select(\"customer_id\").unique()\n    extreme_customers = extreme_customers.collect()\n\n    print(\n        f\"\"\"\n        Cutoff Values for {quantile*100}th Percentiles:\n        -----------------------------------\n        Total items bought:    {total_items_99:.0f} items\n\n        -----------------------------------\n        Removed Customers:     {len(extreme_customers):,}\n        \"\"\"\n    )\n\n    return grouped_df.collect(), extreme_customers\n\ndef train_test_split(\n    train_df: pl.DataFrame,\n    test_df: pl.DataFrame,\n    subset: int = None,\n    train_subsample_percentage: float = None,\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits data into train, validation, and test sets with optional subsampling.\n\n    The function performs the following operations:\n    1. Optional subsampling of both train and test data\n    2. Optional percentage-based subsampling of training data\n    3. Creates a validation set from 10% of the training data\n\n    Args:\n        train_df (pl.DataFrame): Training dataset.\n        test_df (pl.DataFrame): Test dataset.\n        subset (int, optional): If provided, limits both train and test sets to first n rows. \n            Defaults to None.\n        train_subsample_percentage (float, optional): If provided, randomly samples this percentage \n            of training data. Defaults to None.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (90% of training data after subsampling)\n            - val_df: Validation dataset (10% of training data)\n            - test_df: Test dataset (potentially subsampled)\n\n    Notes:\n        If both subset and train_subsample_percentage are provided, subset is applied first.\n        The validation set is always 10% of the remaining training data after any subsampling.\n    \"\"\"\n\n    if subset is not None:\n        train_df = train_df[:subset]\n        test_df = test_df[:subset]\n    elif train_subsample_percentage is not None:\n        sampled_indices = global_rng.choice(\n            len(train_df),\n            size=int(train_subsample_percentage * len(train_df)),\n            replace=False,\n        )\n        train_df = train_df[sampled_indices]\n\n    # Train-val-split\n    # Calculate 10% of the length of the array\n    sampled_indices = global_rng.choice(\n        len(train_df), size=int(0.1 * len(train_df)), replace=False\n    )\n    val_df = train_df[sampled_indices]\n    train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n\n    return train_df, val_df, test_df\n\ndef map_article_ids(df: pl.DataFrame, data_path: Path) -> pl.DataFrame:\n    \"\"\"Maps article IDs to new running IDs using a mapping dictionary from JSON.\n\n    Args:\n        df (pl.DataFrame): DataFrame with 'article_id' column to be mapped.\n        data_path (Path): Path to directory with 'running_id_dict.json' containing ID mappings.\n\n    Returns:\n        pl.DataFrame: DataFrame with mapped article IDs, sorted by new IDs. Non-mapped articles are removed.\n    \"\"\"\n    with open(data_path / \"running_id_dict.json\", \"r\") as f:\n        data = json.load(f)\n    article_id_dict = data[\"combined\"]\n\n    mapping_df = pl.DataFrame(\n        {\n            \"old_id\": list(article_id_dict.keys()),\n            \"new_id\": list(article_id_dict.values()),\n        },\n        schema_overrides={\"old_id\": pl.Int32, \"new_id\": pl.Int32},\n    )\n\n    # Join and select\n    df = df.join(\n        mapping_df, left_on=\"article_id\", right_on=\"old_id\", how=\"inner\"\n    ).select(\n        pl.col(\"new_id\").alias(\"article_id\"),\n        pl.all().exclude([\"article_id\", \"old_id\", \"new_id\"]),\n    )\n    df = df.sort(\"article_id\")\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from pathlib import Path\n#from data_processing.customer_df.customer_df import get_customer_df_benchmarks\n#from data_processing.transaction_df.transaction_df import get_tx_article_dfs\nimport polars as pl\n\n\ndef expand_list_columns(\n    df: pl.DataFrame, date_col: str = \"days_before_lst\", num_col: str = \"num_items_lst\"\n) -> pl.DataFrame:\n    \"\"\"\n    Expand a Polars DataFrame by repeating each element in a list column according to\n    the counts specified in another list column.\n\n    Args:\n        df: Input Polars DataFrame with list columns\n        date_col: Name of the column containing the lists to be expanded\n        num_col: Name of the column containing lists of counts\n\n    Returns:\n        A new Polars DataFrame where the list elements in date_col have been expanded\n    \"\"\"\n    expanded = df.with_columns(\n        pl.struct([date_col, num_col])\n        .map_elements(\n            lambda x: [\n                date\n                for date, count in zip(x[date_col], x[num_col])\n                for _ in range(count)\n            ]\n        )\n        .alias(date_col)\n    )\n\n    return expanded\n\n\ndef add_benchmark_tx_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Creates benchmark transaction features from aggregated customer transaction data.\n\n    Args:\n        df: A Polars DataFrame containing aggregated transaction data with list columns\n            including total_price_lst, num_items_lst, days_before_lst, price_lst,\n            and CLV_label.\n\n    Returns:\n        pl.DataFrame: A DataFrame with derived features including:\n            - total_spent: Sum of all transaction amounts\n            - total_purchases: Count of transactions\n            - total_items: Sum of items purchased\n            - days_since_last_purchase: Days since most recent transaction\n            - days_since_first_purchase: Days since first transaction\n            - avg_spent_per_transaction: Mean transaction amount\n            - avg_items_per_transaction: Mean items per transaction\n            - avg_days_between: Mean days between transactions\n            - regression_label: CLV label for regression\n            - classification_label: Binary CLV label (>0)\n\n    Note:\n        The avg_days_between calculation may return None for customers with single\n        transactions, which is handled by tree-based algorithms.\n    \"\"\"\n    return df.select(\n        \"customer_id\",\n        pl.col(\"total_price_lst\").list.sum().alias(\"total_spent\"),\n        pl.col(\"total_price_lst\").list.len().alias(\"total_purchases\"),\n        pl.col(\"num_items_lst\").list.sum().alias(\"total_items\"),\n        pl.col(\"days_before_lst\").list.get(-1).alias(\"days_since_last_purchase\"),\n        pl.col(\"days_before_lst\").list.get(0).alias(\"days_since_first_purchase\"),\n        pl.col(\"price_lst\").list.mean().alias(\"avg_spent_per_transaction\"),\n        (\n            pl.col(\"num_items_lst\")\n            .list.mean()\n            .cast(pl.Float32)\n            .alias(\"avg_items_per_transaction\")\n        ),\n        # Code below returns None values for customers with single Tx\n        # Tree algos should be able to handle this\n        (\n            pl.col(\"days_before_lst\")\n            .list.diff(null_behavior=\"drop\")\n            .list.mean()\n            .mul(-1)\n            .cast(pl.Float32)\n            .alias(\"avg_days_between\")\n        ),\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    )\n\n\ndef process_dataframe(df: pl.DataFrame, max_length: int = 20) -> pl.DataFrame:\n    \"\"\"Processes a polars DataFrame by expanding list columns and selecting specific columns with transformations.\n\n    This function performs several operations on the input DataFrame:\n    1. Expands list columns using the expand_list_columns function\n    2. Selects and renames specific columns\n    3. Truncates list columns to a maximum length\n\n    Args:\n        df: A polars DataFrame containing customer transaction data\n        max_length: Maximum number of elements to keep in list columns (default: 20)\n\n    Returns:\n        A processed polars DataFrame with the following columns:\n            - customer_id: Customer identifier\n            - days_before_lst: Truncated list of days before some reference date\n            - articles_ids_lst: Truncated list of article identifiers\n            - regression_label: CLV label for regression tasks\n            - classification_label: Binary classification label derived from CLV\n    \"\"\"\n    df = expand_list_columns(df, date_col=\"days_before_lst\", num_col=\"num_items_lst\")\n    return df.select(\n        \"customer_id\",\n        \"days_before_lst\",\n        \"articles_ids_lst\",\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    ).with_columns(\n        pl.col(\"days_before_lst\").list.tail(max_length),\n        pl.col(\"articles_ids_lst\").list.tail(max_length),\n    )\n\n\ndef get_benchmark_dfs(\n    data_path: Path, config: dict\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Creates benchmark train, validation, and test datasets with transaction and customer features.\n\n    Args:\n        data_path: Path object pointing to the data directory\n        config: Dictionary containing configuration parameters for data processing\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: A tuple containing:\n            - train_df: Training dataset with benchmark features\n            - val_df: Validation dataset with benchmark features\n            - test_df: Test dataset with benchmark features\n\n        Each DataFrame contains transaction-derived features joined with customer features.\n    \"\"\"\n    train_article, val_article, test_article = get_tx_article_dfs(\n        data_path=data_path,\n        config=config,\n        cols_to_aggregate=[\n            \"date\",\n            \"days_before\",\n            \"article_ids\",\n            \"sales_channel_ids\",\n            \"total_price\",\n            \"prices\",\n            \"num_items\",\n        ],\n        keep_customer_id=True,\n    )\n\n    customer_df = get_customer_df_benchmarks(data_path=data_path, config=config)\n\n    train_df = process_dataframe(\n        df=train_article, max_length=config[\"max_length\"]\n    ).join(customer_df, on=\"customer_id\", how=\"left\")\n    val_df = process_dataframe(df=val_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n    test_df = process_dataframe(df=test_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n\n    return train_df, val_df, test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\n#from pathlib import Path\n\n\ndef get_customer_df_benchmarks(data_path: Path, config: dict):\n    \"\"\"Processes customer data with age grouping and zip code mapping.\n\n    Args:\n        data_path (Path): Path to directory containing 'customers.csv' and 'zip_code_count.csv'.\n        config (dict): Configuration with 'min_zip_code_count'. Updated with 'num_age_groups' and 'num_zip_codes'.\n\n    Returns:\n        pl.DataFrame: Processed DataFrame with customer_id, age_group (0-6), and mapped zip_code_id.\n    \"\"\"\n    file_path = data_path / \"customers.csv\"\n    df = pl.scan_csv(file_path).select(\n        (\n            \"customer_id\",\n            pl.col(\"age\").fill_null(strategy=\"mean\"),\n            \"postal_code\",\n        )\n    )\n\n    # df = df.with_columns(\n    #     [\n    #         pl.when(pl.col(\"age\").is_null())\n    #         .then(0)\n    #         .when(pl.col(\"age\") < 25)\n    #         .then(1)\n    #         .when(pl.col(\"age\").is_between(25, 34))\n    #         .then(2)\n    #         .when(pl.col(\"age\").is_between(35, 44))\n    #         .then(3)\n    #         .when(pl.col(\"age\").is_between(45, 54))\n    #         .then(4)\n    #         .when(pl.col(\"age\").is_between(55, 64))\n    #         .then(5)\n    #         .otherwise(6)\n    #         .alias(\"age_group\")\n    #     ]\n    # )\n    # config[\"num_age_groups\"] = 7\n\n    return df.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from datetime import datetime\n#from pathlib import Path\n#import polars as pl\n\n#from data_processing.utils.utils_transaction_df import (\n #   filter_purchases_purchases_per_month_pl,\n  #  map_article_ids,\n   # train_test_split,\n#)\n\n\ndef generate_clv_data_pl(\n    df: pl.DataFrame,\n    agg_df: pl.DataFrame,\n    label_threshold: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    log_clv: bool = False,\n):\n    \"\"\"Generates Customer Lifetime Value (CLV) data from transaction dataframe.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe containing customer purchases.\n        agg_df (pl.DataFrame): Aggregated dataframe containing customer data.\n        label_threshold (datetime.date): Start date for CLV calculation period.\n        pred_end (datetime.date): End date for CLV calculation period.\n        clv_periods (list): List of periods for CLV calculation (currently supports single period only).\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated dataframe with added CLV calculations.\n\n    Raises:\n        ValueError: If more than one CLV period is provided.\n    \"\"\"\n    if len(clv_periods) > 1:\n        raise ValueError(\"CLV periods should be a single number for now.\")\n\n    # Filter transactions between label_threshold and end_date for each period\n    filtered_df = df.filter(\n        (pl.col(\"date\") >= label_threshold) & (pl.col(\"date\") <= pred_end)\n    )\n\n    # Sum total_price for the filtered transactions by customer_id. This is the CLV\n    summed_period_df = filtered_df.group_by(\"customer_id\").agg(\n        pl.sum(\"total_price\").round(2).alias(f\"CLV_label\")\n    )\n    if log_clv:\n        summed_period_df = summed_period_df.with_columns(\n            pl.col(f\"CLV_label\").log1p().round(2).alias(f\"CLV_label\")\n        )\n\n    agg_df = agg_df.join(summed_period_df, on=\"customer_id\", how=\"left\")\n\n    agg_df = agg_df.fill_null(0)\n    return agg_df\n\n\ndef group_and_convert_df_pl(\n    df: pl.DataFrame,\n    label_start_date: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"num_items\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> pl.DataFrame:\n    \"\"\"Groups and converts transaction data into aggregated customer-level features.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        label_start_date (datetime.date): Start date for clv label period.\n        pred_end (datetime.date): End date for prediction period.\n        clv_periods (list): List of periods for CLV calculation.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated customer-level dataframe.\n\n    Raises:\n        ValueError: If required columns (days_before, article_ids, num_items) are missing from cols_to_aggregate.\n    \"\"\"\n\n    if any(\n        col not in cols_to_aggregate\n        for col in [\"days_before\", \"article_ids\", \"num_items\"]\n    ):\n        raise ValueError(\n            \"The columns days_before, article_ids, and num_items are required \"\n            \"for the aggregation\"\n        )\n\n    mapping = {\n        \"date\": \"date_lst\",\n        \"days_before\": \"days_before_lst\",\n        \"article_ids\": \"articles_ids_lst\",\n        \"sales_channel_ids\": \"sales_channel_id_lst\",\n        \"total_price\": \"total_price_lst\",\n        \"prices\": \"price_lst\",\n        \"num_items\": \"num_items_lst\",\n    }\n\n    agg_df = (\n        df.filter(pl.col(\"date\") < label_start_date)\n        .with_columns(\n            (label_start_date - pl.col(\"date\"))\n            .dt.total_days()\n            .cast(pl.Int32)\n            .alias(\"days_before\"),\n            (\n                pl.col(\"sales_channel_ids\")\n                .cast(pl.List(pl.Int32))\n                .alias(\"sales_channel_ids\")\n            ),\n            pl.col(\"article_ids\").cast(pl.List(pl.Int32)).alias(\"article_ids\"),\n        )\n        .sort(\"customer_id\", \"date\")\n        .group_by(\"customer_id\")\n        .agg(\n            pl.col(\"date\").explode().alias(\"date_lst\"),\n            pl.col(\"days_before\").explode().alias(\"days_before_lst\"),\n            pl.col(\"article_ids\").explode().alias(\"articles_ids_lst\"),\n            pl.concat_list(pl.col(\"sales_channel_ids\")).alias(\"sales_channel_id_lst\"),\n            pl.col(\"total_price\").explode().alias(\"total_price_lst\"),\n            pl.col(\"prices\").explode().alias(\"price_lst\"),\n            pl.col(\"num_items\").explode().alias(\"num_items_lst\"),\n        )\n    )\n\n    if clv_periods is not None:\n        agg_df = generate_clv_data_pl(\n            df=df,\n            agg_df=agg_df,\n            label_threshold=label_start_date,\n            pred_end=pred_end,\n            clv_periods=clv_periods,\n            log_clv=log_clv,\n        )\n\n    # Drop columns which are not to be aggregated\n    cols_to_drop = [v for k, v in mapping.items() if k not in cols_to_aggregate]\n    if not keep_customer_id:\n        cols_to_drop.append(\"customer_id\")\n    agg_df = agg_df.drop(*cols_to_drop)\n\n    return agg_df\n\n\ndef split_df_and_group_pl(\n    df: pl.DataFrame,\n    clv_periods: list,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits transaction data into training and test sets and performs aggregation.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        clv_periods (list): List of periods for CLV calculation.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Aggregated training dataset\n            - test_df: Aggregated test dataset\n    \"\"\"\n\n    train_begin = datetime.strptime(config.get(\"train_begin\"), \"%Y-%m-%d\")\n    train_label_start = datetime.strptime(config.get(\"train_label_begin\"), \"%Y-%m-%d\")\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    test_begin = datetime.strptime(config.get(\"test_begin\"), \"%Y-%m-%d\")\n    test_label_start = datetime.strptime(config.get(\"test_label_begin\"), \"%Y-%m-%d\")\n    test_end = datetime.strptime(config.get(\"test_end\"), \"%Y-%m-%d\")\n\n    # Creating the training DataFrame by filtering dates up to `train_end`\n    train_df = df.filter(\n        (pl.col(\"date\") <= train_end) & (pl.col(\"date\") >= train_begin)\n    )\n\n    train_df = group_and_convert_df_pl(\n        df=train_df,\n        label_start_date=train_label_start,\n        pred_end=train_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    # Creating the test DataFrame by filtering dates after `test_begin`\n    test_df = df.filter((pl.col(\"date\") >= test_begin) & (pl.col(\"date\") <= test_end))\n\n    test_df = group_and_convert_df_pl(\n        df=test_df,\n        label_start_date=test_label_start,\n        pred_end=test_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    return train_df, test_df\n\n\ndef load_data_rem_outlier_pl(\n    data_path: Path, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Loads transaction data, applies price scaling, and removes outliers.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data parquet file.\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group data by sales channel ID. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Processed transaction dataframe\n            - extreme_customers: Dataframe of customers identified as outliers\n    \"\"\"\n    file_path = data_path / \"transactions_polars.parquet\"\n    df_pl = pl.read_parquet(file_path)\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"t_dat\").alias(\"date\").cast(pl.Date), pl.col(\"article_id\").cast(pl.Int32)\n    )\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"price\").mul(590).cast(pl.Float32).round(2).alias(\"price\")\n    )\n\n    # Map article ids to running ids so that they match with feature matrix\n    df_pl = map_article_ids(df=df_pl, data_path=data_path)\n\n    grouped_df, extreme_customers = filter_purchases_purchases_per_month_pl(\n        df_pl, train_end=train_end, group_by_channel_id=group_by_channel_id\n    )\n\n    return grouped_df, extreme_customers\n\n\ndef get_customer_train_test_articles_pl(\n    data_path: Path,\n    config: dict,\n    clv_periods: list = None,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Processes customer transaction data into train and test sets with article information.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data.\n        config (dict): Configuration dictionary for data processing parameters.\n        clv_periods (list, optional): List of periods for CLV calculation. Defaults to None.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Processed training dataset with article information\n            - test_df: Processed test dataset with article information\n    \"\"\"\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    grouped_df, extreme_customers = load_data_rem_outlier_pl(\n        data_path=data_path, train_end=train_end\n    )\n\n    train_df, test_df = split_df_and_group_pl(\n        df=grouped_df,\n        clv_periods=clv_periods,\n        config=config,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=True,\n        log_clv=config.get(\"log_clv\", False),\n    )\n\n    train_df = train_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n    test_df = test_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n\n    if not keep_customer_id:\n        train_df = train_df.drop(\"customer_id\")\n        test_df = test_df.drop(\"customer_id\")\n\n    return train_df, test_df\n\n\ndef get_tx_article_dfs(\n    data_path: Path,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Creates train, validation, and test datasets with optional subsampling.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data files.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Transaction columns to include in output.\n        keep_customer_id (bool, optional): Whether to retain customer_id column.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (subset of original training data)\n            - val_df: Validation dataset (10% of original training data)\n            - test_df: Test dataset (optionally subsampled)\n    \"\"\"\n    \"\"\"\n    Columns of dfs:\n        - customer_id\n        - date_lst (list[date]): Dates of each transaction\n        - days_before_lst (list[int]): Number of days between start of prediction and date of transction\n        - articles_ids_lst (list[int]): Flattened list of all items a customer purchased \n        - sales_channel_id_lst (list[list[int]]): Sales channel of a transaction (repeated for each item within a transaction)\n        - total_price_lst (list[float]): Value of each transaction\n        - price_lst (list[float]): Flattened list of prices of all items customer purchased\n        - num_items_lst (list[int]): Number of items in each transaction\n        - CLV_label (float): Sales in prediction period (label to be used)\n    \"\"\"\n    train_df, test_df = get_customer_train_test_articles_pl(\n        data_path=data_path,\n        config=config,\n        clv_periods=config.get(\"clv_periods\", [6]),\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n    )\n    train_df, val_df, test_df = train_test_split(\n        train_df=train_df,\n        test_df=test_df,\n        subset=config.get(\"subset\"),\n        train_subsample_percentage=config.get(\"train_subsample_percentage\"),\n    )\n    return train_df, val_df, test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"train_begin\": \"2018-09-20\",\n    \"train_label_begin\": \"2019-09-20\",\n    \"train_end\": \"2020-03-17\",\n    \"test_begin\": \"2019-03-19\",\n    \"test_label_begin\": \"2020-03-18\",\n    \"test_end\": \"2020-09-13\",\n    \"min_zip_code_count\": 3,\n    \"date_aggregation\": \"daily\",\n    \"group_by_channel_id\": False,\n    \"log_clv\": False,\n    \"clv_periods\": [6],\n    \"subset\": None,\n    \"train_subsample_percentage\": None,\n    \"max_length\":20, # DEFINE HOW MANY ITEMS ARE TO BE CONSIDERED IN TRANSFORMER SEQUENCE\n}\n# data_path = Path(\"/kaggle/input/hm-dataset/data/data\")\ndata_path = Path(\"/kaggle/input/data/data/\")\n\nprint(10 * \"#\", \" Loading data \", 10 * \"#\")\ntrain_df, val_df, test_df = get_benchmark_dfs(data_path, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.special  # for hyp2f1\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install lifetimes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.special  # for hyp2f1\nimport numpy as np\nimport pandas as pd\n\n# If you are using Polars, import it (optional)\ntry:\n    import polars as pl\nexcept ImportError:\n    pl = None\n\n\nclass BGNBDModel(nn.Module):\n    \"\"\"\n    BG/NBD model for transaction frequency:\n      - r, alpha : Gamma-Poisson mixture for transaction rates\n      - a, b     : Beta-Geometric mixture for dropout\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(BGNBDModel, self).__init__()\n        if init_params is None:\n            init_params = {'r': 1.0, 'alpha': 1.0, 'a': 1.0, 'b': 1.0}\n        self.log_r = nn.Parameter(torch.log(torch.tensor(init_params['r'], dtype=torch.float32)))\n        self.log_alpha = nn.Parameter(torch.log(torch.tensor(init_params['alpha'], dtype=torch.float32)))\n        self.log_a = nn.Parameter(torch.log(torch.tensor(init_params['a'], dtype=torch.float32)))\n        self.log_b = nn.Parameter(torch.log(torch.tensor(init_params['b'], dtype=torch.float32)))\n\n    def forward(self, x, t_x, T):\n        \"\"\"\n        Compute the log-likelihood (LL) for each customer under BG/NBD.\n        \"\"\"\n        # Ensure inputs are floats\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        # Recover parameters\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        ll = torch.empty_like(x)\n\n        mask0 = (x == 0)\n        mask1 = (x > 0)\n\n        # --- x = 0 case ---\n        if mask0.any():\n            T0 = T[mask0]\n            ll0 = (r * torch.log(alpha)\n                   - r * torch.log(alpha + T0)\n                   + torch.log(b)\n                   - torch.log(a + b))\n            ll[mask0] = ll0\n\n        # --- x > 0 case ---\n        if mask1.any():\n            x1   = x[mask1]\n            t_x1 = t_x[mask1]\n            T1   = T[mask1]\n\n            term1 = ( torch.lgamma(r + x1)\n                      - torch.lgamma(r)\n                      - torch.lgamma(x1 + 1) )\n            term2 = r * ( torch.log(alpha) - torch.log(alpha + T1) )\n            term3 = x1 * ( torch.log(T1 - t_x1) - torch.log(alpha + T1) )\n            term4 = ( torch.log(a)\n                      + torch.lgamma(a + b)\n                      - torch.lgamma(a)\n                      - torch.lgamma(a + b + x1)\n                      + torch.lgamma(a + x1) )\n            z = (T1 - t_x1) / (alpha + T1)\n            # hypergeometric term\n            hyp_val = torch.special.hyp2f1(r + x1, a, a + b + x1, z)\n            term5 = torch.log(hyp_val + 1e-30)\n\n            ll1 = term1 + term2 + term3 + term4 + term5\n            ll[mask1] = ll1\n\n        return ll\n\n    def negative_log_likelihood(self, x, t_x, T):\n        ll = self.forward(x, t_x, T)\n        return -ll.sum()\n\n    def expected_transactions(self, x, t_x, T, t_future=10.0):\n        \"\"\"\n        Approximate E[# of transactions in (T, T+t_future)].\n        We'll use a simplified approximation:\n            E[X(t_future)|...] ≈ p_alive * ((r + x)/(alpha + T)) * t_future\n        where p_alive is computed using a formula from Hardie's notes.\n        \"\"\"\n        x   = x.float()\n        t_x = t_x.float()\n        T   = T.float()\n\n        r = torch.exp(self.log_r)\n        alpha = torch.exp(self.log_alpha)\n        a = torch.exp(self.log_a)\n        b = torch.exp(self.log_b)\n\n        # Compute log_factor for p_alive\n        log_factor = ( torch.lgamma(a + 1)\n                       + torch.lgamma(b + x)\n                       - torch.lgamma(a)\n                       - torch.lgamma(b + x + 1) )\n        log_factor += (r + x) * ( torch.log(alpha + T) - torch.log(alpha + t_x + 1e-8) )\n        factor = torch.exp(log_factor)\n        p_alive = 1.0 / (1.0 + factor)\n\n        return p_alive * ((r + x) / (alpha + T + 1e-8)) * t_future\n\n\nclass GammaGammaModel(nn.Module):\n    \"\"\"\n    Gamma–Gamma model for monetary value.\n    \"\"\"\n    def __init__(self, init_params=None):\n        super(GammaGammaModel, self).__init__()\n        if init_params is None:\n            init_params = {'p': 1.0, 'q': 1.0, 'v': 1.0}\n        self.log_p = nn.Parameter(torch.log(torch.tensor(init_params['p'], dtype=torch.float32)))\n        self.log_q = nn.Parameter(torch.log(torch.tensor(init_params['q'], dtype=torch.float32)))\n        self.log_v = nn.Parameter(torch.log(torch.tensor(init_params['v'], dtype=torch.float32)))\n\n    def forward(self, x, m):\n        \"\"\"\n        Compute the log-likelihood for the Gamma–Gamma model.\n        \"\"\"\n        x = x.float()\n        m = m.float()\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n        eps = 1e-30\n        term1 = torch.lgamma(p + q*x) - torch.lgamma(p) - torch.lgamma(q*x + eps)\n        term2 = p * torch.log(v + eps)\n        term3 = (q - 1) * x * torch.log(m + eps)\n        term4 = - (p + q*x) * torch.log(v + x*m + eps)\n        return term1 + term2 + term3 + term4\n\n    def negative_log_likelihood(self, x, m):\n        ll = self.forward(x, m)\n        return -ll.sum()\n\n    def conditional_expected_value(self, x, m):\n        \"\"\"\n        Compute E[m|x, m] = (p + q*x) / (v + x*m).\n        \"\"\"\n        x = x.float()\n        m = m.float()\n        p = torch.exp(self.log_p)\n        q = torch.exp(self.log_q)\n        v = torch.exp(self.log_v)\n        eps = 1e-8\n        return (p + q * x) / (v + x*m + eps)\n\n\nclass CompositeCLVModel(nn.Module):\n    \"\"\"\n    Composite model that predicts future CLV as:\n         Predicted CLV = E[# future transactions] * E[monetary value]\n    \"\"\"\n    def __init__(self, init_bgnbd=None, init_gg=None, t_future=10.0):\n        super(CompositeCLVModel, self).__init__()\n        self.bgnbd = BGNBDModel(init_params=init_bgnbd)\n        self.ggamma = GammaGammaModel(init_params=init_gg)\n        self.t_future = t_future\n\n    def forward(self, x, t_x, T, m):\n        count_pred = self.bgnbd.expected_transactions(x, t_x, T, self.t_future)\n        value_pred = self.ggamma.conditional_expected_value(x, m)\n        return count_pred * value_pred\n\n    def loss_mse(self, x, t_x, T, m, actual_spend):\n        pred = self.forward(x, t_x, T, m)\n        return torch.mean((pred - actual_spend)**2)\n\n\ndef parse_x_t_x_T(row):\n    \"\"\"\n    Parse (x, t_x, T) from 'days_before_lst'.\n    \"\"\"\n    days_list = row['days_before_lst']\n    if not isinstance(days_list, list) or len(days_list) == 0:\n        return 0, 0.0, 0.0\n    else:\n        x = len(days_list)\n        T = float(sum(days_list))\n        t_x = float(days_list[-1])\n        return x, t_x, T\n\ndef parse_avg_monetary_value(row):\n    \"\"\"\n    Parse a dummy average monetary value from 'articles_ids_lst'.\n    Here we define: m = 20 + 0.1 * (number of articles).\n    \"\"\"\n    arts = row['articles_ids_lst']\n    if not isinstance(arts, list) or len(arts) == 0:\n        return 20.0\n    else:\n        return 20.0 + 0.1 * len(arts)\n\ndef build_tensor_dataset(df):\n    \"\"\"\n    Build PyTorch tensors from the DataFrame.\n    Expects columns:\n      - 'days_before_lst'\n      - 'articles_ids_lst'\n      - 'regression_label'\n    \"\"\"\n    # If the DataFrame is a Polars DataFrame, convert it to Pandas.\n    if not hasattr(df, \"iterrows\"):\n        df = df.to_pandas()\n        \n    x_list, t_x_list, T_list, m_list, spend_list = [], [], [], [], []\n    for _, row in df.iterrows():\n        x_val, tx_val, T_val = parse_x_t_x_T(row)\n        m_val = parse_avg_monetary_value(row)\n        clv = row.get('regression_label', 0.0)\n        x_list.append(x_val)\n        t_x_list.append(tx_val)\n        T_list.append(T_val)\n        m_list.append(m_val)\n        spend_list.append(clv)\n    x_ten = torch.tensor(x_list, dtype=torch.float32)\n    t_x_ten = torch.tensor(t_x_list, dtype=torch.float32)\n    T_ten = torch.tensor(T_list, dtype=torch.float32)\n    m_ten = torch.tensor(m_list, dtype=torch.float32)\n    spend_ten = torch.tensor(spend_list, dtype=torch.float32)\n    return x_ten, t_x_ten, T_ten, m_ten, spend_ten\n\n\ndef train_and_validate(train_df, val_df, test_df):\n    # Build tensors from DataFrames\n    x_train, t_x_train, T_train, m_train, spend_train = build_tensor_dataset(train_df)\n    x_val, t_x_val, T_val, m_val, spend_val = build_tensor_dataset(val_df)\n    x_test, t_x_test, T_test, m_test, spend_test = build_tensor_dataset(test_df)\n\n    # Create composite model\n    model = CompositeCLVModel(\n        init_bgnbd={'r':1.0, 'alpha':1.0, 'a':1.0, 'b':1.0},\n        init_gg={'p':1.0, 'q':1.0, 'v':1.0},\n        t_future=10.0\n    )\n    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n    n_epochs = 500\n\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        loss = model.loss_mse(x_train, t_x_train, T_train, m_train, spend_train)\n        loss.backward()\n        optimizer.step()\n        if (epoch+1) % 100 == 0:\n            with torch.no_grad():\n                val_preds = model.forward(x_val, t_x_val, T_val, m_val)\n                val_loss = torch.mean((val_preds - spend_val)**2)\n            print(f\"Epoch {epoch+1}/{n_epochs} | Train MSE: {loss.item():.4f} | Val MSE: {val_loss.item():.4f}\")\n\n    with torch.no_grad():\n        test_preds = model.forward(x_test, t_x_test, T_test, m_test)\n        test_mse = torch.mean((test_preds - spend_test)**2).item()\n    print(f\"\\nFinal Test MSE: {test_mse:.4f}\")\n\n    # Print learned parameters\n    print(\"\\nLearned BG/NBD parameters:\")\n    print(\"r     =\", torch.exp(model.bgnbd.log_r).item())\n    print(\"alpha =\", torch.exp(model.bgnbd.log_alpha).item())\n    print(\"a     =\", torch.exp(model.bgnbd.log_a).item())\n    print(\"b     =\", torch.exp(model.bgnbd.log_b).item())\n\n    print(\"\\nLearned Gamma–Gamma parameters:\")\n    print(\"p =\", torch.exp(model.ggamma.log_p).item())\n    print(\"q =\", torch.exp(model.ggamma.log_q).item())\n    print(\"v =\", torch.exp(model.ggamma.log_v).item())\n\n\ntrain_and_validate(train_df, val_df, test_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import official BG–NBD and Gamma–Gamma from lifetimes:\nfrom lifetimes import BetaGeoFitter, GammaGammaFitter\n\n# Import Pareto–NBD and MBG/NBD fitters from lifetimes (assumed to be available in your version)\nfrom lifetimes import ParetoNBDFitter, ModifiedBetaGeoFitter\n\n# --- Utility: Convert to Pandas if needed ---\ndef to_pandas(df):\n    if not hasattr(df, \"iterrows\"):\n        return df.to_pandas()\n    return df\n\n# --- Feature Extraction ---\ndef extract_history_features(df):\n    \"\"\"\n    For each customer, extract:\n      - frequency: number of repeat purchases = (# transactions - 1)\n      - recency: last transaction – first transaction (in days)\n      - T: calibration period (here defined as recency; adjust as needed)\n      - monetary_value: estimate = 20 + 0.1 * (number of articles in articles_ids_lst)\n      - regression_label: true future spend\n    \"\"\"\n    frequencies, recencies, Ts, monetary_values = [], [], [], []\n    df = to_pandas(df)\n    for idx, row in df.iterrows():\n        days = row['days_before_lst']\n        if not isinstance(days, list) or len(days)==0:\n            frequencies.append(0)\n            recencies.append(0.0)\n            Ts.append(0.0)\n        else:\n            x = len(days)\n            frequencies.append(max(x - 1, 0))\n            recencies.append(days[-1] - days[0])\n            Ts.append(days[-1] - days[0])\n        arts = row['articles_ids_lst']\n        if not isinstance(arts, list) or len(arts)==0:\n            monetary_values.append(20.0)\n        else:\n            monetary_values.append(20.0 + 0.1 * len(arts))\n    df_out = pd.DataFrame({\n        'frequency': frequencies,\n        'recency': recencies,\n        'T': Ts,\n        'monetary_value': monetary_values,\n        'regression_label': df['regression_label']\n    })\n    return df_out\n\n# --- Predicted Future Transactions and Monetary Value ---\ndef predict_future_transactions(fitter, frequency, recency, T, t_future):\n    \"\"\"\n    Compute:\n      E[X(t_future)|frequency, recency, T] = conditional_probability_alive *\n          ((r + frequency) / (alpha + T)) * t_future\n    This formula is used in BetaGeoFitter and assumed to be similarly available in ParetoNBDFitter and MBGNBDFitter.\n    \"\"\"\n    p_alive = fitter.conditional_probability_alive(frequency, recency, T)\n    return p_alive * ((fitter.r_ + frequency) / (fitter.alpha_ + T)) * t_future\n\ndef predict_monetary_value(ggf, frequency, monetary_value):\n    \"\"\"\n    Compute:\n       E[m|frequency, monetary_value] = (p + q*frequency) / (v + frequency * monetary_value)\n    \"\"\"\n    return (ggf.p_ + ggf.q_ * frequency) / (ggf.v_ + frequency * monetary_value + 1e-8)\n\ndef compute_predicted_clv(fitter, ggf, features_df, t_future):\n    \"\"\"\n    For each row in features_df, compute:\n      predicted_CLV = E[X(t_future)] * E[m]\n    \"\"\"\n    predictions = []\n    for idx, row in features_df.iterrows():\n        freq = row['frequency']\n        rec = row['recency']\n        T_val = row['T']\n        mon_val = row['monetary_value']\n        exp_tx = predict_future_transactions(fitter, freq, rec, T_val, t_future)\n        exp_m = predict_monetary_value(ggf, freq, mon_val)\n        predictions.append(exp_tx * exp_m)\n    df_out = features_df.copy()\n    df_out['predicted_clv'] = predictions\n    return df_out\n\n# --- Main Pipeline Function ---\ndef main_pipeline(train_df, val_df, test_df, t_future=12):\n    \"\"\"\n    Calibrate the different transaction models (BG–NBD, Pareto–NBD, MBG/NBD) on train_df,\n    calibrate a Gamma–Gamma model on train_df (for customers with frequency > 0),\n    then compute predicted CLV for each dataset.\n    Returns a dictionary with results for each model.\n    \"\"\"\n    # Convert to Pandas and extract features\n    train_features = extract_history_features(train_df)\n    val_features = extract_history_features(val_df)\n    test_features = extract_history_features(test_df)\n    \n    results = {}\n    \n    # --- BG–NBD + Gamma–Gamma ---\n    bgf = BetaGeoFitter(penalizer_coef=0.0)\n    bgf.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    mask = train_features['frequency'] > 0\n    ggf_bg = GammaGammaFitter(penalizer_coef=0.0)\n    ggf_bg.fit(train_features.loc[mask, 'monetary_value'], train_features.loc[mask, 'frequency'])\n    results['BG-NBD'] = {\n        'train': compute_predicted_clv(bgf, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(bgf, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(bgf, ggf_bg, test_features, t_future)\n    }\n    \n    # --- Pareto–NBD + Gamma–Gamma ---\n    pareto_fitter = ParetoNBDFitter(penalizer_coef=0.0)\n    pareto_fitter.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    # Use same GammaGammaFitter as above (or fit separately if desired)\n    results['Pareto-NBD'] = {\n        'train': compute_predicted_clv(pareto_fitter, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(pareto_fitter, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(pareto_fitter, ggf_bg, test_features, t_future)\n    }\n    \n    # --- MBG/NBD + Gamma–Gamma ---\n    mbg_fitter = MBGNBDFitter(penalizer_coef=0.0)\n    mbg_fitter.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    results['MBG-NBD'] = {\n        'train': compute_predicted_clv(mbg_fitter, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(mbg_fitter, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(mbg_fitter, ggf_bg, test_features, t_future)\n    }\n    \n    return results\n\n# --- Example Usage ---\n\n    # Run the pipeline\nresults = main_pipeline(train_df, val_df, test_df, t_future=12)\n    \nprint(\"BG–NBD + Gamma–Gamma Predictions (Test):\")\nprint(results['BG-NBD']['test'])\nprint(\"\\nPareto–NBD + Gamma–Gamma Predictions (Test):\")\nprint(results['Pareto-NBD']['test'])\nprint(\"\\nMBG–NBD + Gamma–Gamma Predictions (Test):\")\nprint(results['MBG-NBD']['test'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import lifetimes models\nfrom lifetimes import BetaGeoFitter, GammaGammaFitter\n\n# If using Polars, convert to Pandas\ndef to_pandas(df):\n    if not hasattr(df, \"iterrows\"):\n        return df.to_pandas()\n    return df\n\n# --- Feature Extraction ---\ndef extract_history_features(df):\n    \"\"\"\n    For each customer, compute:\n      - frequency: number of repeat purchases (total transactions - 1)\n      - recency: difference between the last and first transaction (in days)\n      - T: observation period (here, set equal to recency)\n      - monetary_value: estimated as 20 + 0.1 * (number of articles)\n    \"\"\"\n    frequencies = []\n    recencies = []\n    Ts = []\n    monetary_values = []\n    \n    df = to_pandas(df)\n    for idx, row in df.iterrows():\n        days = row['days_before_lst']\n        if not isinstance(days, list) or len(days) == 0:\n            frequencies.append(0)\n            recencies.append(0.0)\n            Ts.append(0.0)\n        else:\n            x = len(days)\n            frequencies.append(max(x - 1, 0))\n            recencies.append(days[-1] - days[0])\n            Ts.append(days[-1] - days[0])  # In practice, T can be defined differently\n        arts = row['articles_ids_lst']\n        if not isinstance(arts, list) or len(arts) == 0:\n            monetary_values.append(20.0)\n        else:\n            monetary_values.append(20.0 + 0.1 * len(arts))\n    \n    df_new = pd.DataFrame({\n        'frequency': frequencies,\n        'recency': recencies,\n        'T': Ts,\n        'monetary_value': monetary_values,\n        'regression_label': df['regression_label']\n    })\n    return df_new\n\n# --- CLV Calculation Without customer_lifetime_value ---\ndef compute_predicted_clv(bgf, ggf, frequency, recency, T, monetary_value, t_future):\n    \"\"\"\n    Computes predicted CLV as:\n      predicted_CLV = E[# future txns] * E[monetary value],\n    where:\n      p_alive = bgf.conditional_probability_alive(frequency, recency, T)\n      E[# future txns] = p_alive * ((r + frequency) / (alpha + T)) * t_future\n      E[m] = (ggf.p_ + ggf.q_ * frequency) / (ggf.v_ + frequency * monetary_value)\n    \"\"\"\n    # Compute probability alive (returns a numpy array)\n    p_alive = bgf.conditional_probability_alive(frequency, recency, T)\n    # Expected transactions in future period\n    expected_txns = p_alive * ((bgf.r_ + frequency) / (bgf.alpha_ + T)) * t_future\n    # Expected monetary value\n    expected_m = (ggf.p_ + ggf.q_ * frequency) / (ggf.v_ + frequency * monetary_value + 1e-8)\n    return expected_txns * expected_m\n\ndef compute_clv_custom(train_df, val_df, test_df, t_future=12):\n    \"\"\"\n    Calibrate the BG/NBD and Gamma–Gamma models on train_df,\n    then compute predicted CLV (using our custom function) for train, val, and test.\n    \"\"\"\n    # Convert to Pandas if needed\n    train_df = to_pandas(train_df)\n    val_df = to_pandas(val_df)\n    test_df = to_pandas(test_df)\n    \n    # Extract historical features\n    train_hist = extract_history_features(train_df)\n    val_hist = extract_history_features(val_df)\n    test_hist = extract_history_features(test_df)\n    \n    # Fit BG/NBD on training data\n    bgf = BetaGeoFitter(penalizer_coef=0.0)\n    bgf.fit(train_hist['frequency'], train_hist['recency'], train_hist['T'])\n    \n    # Fit Gamma–Gamma on customers with frequency > 0\n    mask = train_hist['frequency'] > 0\n    ggf = GammaGammaFitter(penalizer_coef=0.0)\n    ggf.fit(train_hist.loc[mask, 'monetary_value'], train_hist.loc[mask, 'frequency'])\n    \n    # Compute predicted CLV for each dataset\n    train_hist['predicted_clv'] = train_hist.apply(\n        lambda row: compute_predicted_clv(bgf, ggf,\n                                          row['frequency'], row['recency'], row['T'],\n                                          row['monetary_value'], t_future), axis=1)\n    val_hist['predicted_clv'] = val_hist.apply(\n        lambda row: compute_predicted_clv(bgf, ggf,\n                                          row['frequency'], row['recency'], row['T'],\n                                          row['monetary_value'], t_future), axis=1)\n    test_hist['predicted_clv'] = test_hist.apply(\n        lambda row: compute_predicted_clv(bgf, ggf,\n                                          row['frequency'], row['recency'], row['T'],\n                                          row['monetary_value'], t_future), axis=1)\n    return train_hist, val_hist, test_hist, bgf, ggf\n\nprint(\"=== CLV Prediction Without customer_lifetime_value ===\")\n\ntrain_hist, val_hist, test_hist, bgf, ggf = compute_clv_custom(train_df, val_df, test_df, t_future=12)\n    \nprint(\"\\n--- Train Data ---\")\nprint(train_hist)\nprint(\"\\n--- Validation Data ---\")\nprint(val_hist)\nprint(\"\\n--- Test Data ---\")\nprint(test_hist)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import the official BG–NBD and Gamma–Gamma models from lifetimes\nfrom lifetimes import BetaGeoFitter, GammaGammaFitter\n\nfrom lifetimes import ParetoNBDFitter, ModifiedBetaGeoFitter\n\n\ndef to_pandas(df):\n    \"\"\"If df is a Polars DataFrame, convert it to Pandas.\"\"\"\n    if not hasattr(df, \"iterrows\"):\n        return df.to_pandas()\n    return df\n\ndef impute_and_extract_features(df):\n    \"\"\"\n    Impute missing values and extract key features for CLV modeling.\n    Expected columns:\n      - days_before_lst: list of transaction times (in days); if NaN, impute as empty list.\n      - articles_ids_lst: list of article IDs; if NaN, impute as empty list.\n      - regression_label: the true future spend (CLV); if NaN, impute as 0.\n      \n    Then, compute:\n      - frequency: # transactions - 1 (if no transaction, 0)\n      - recency: difference between last and first transaction (in days)\n      - T: calibration period (here set equal to recency; adapt if needed)\n      - monetary_value: estimate = 20 + 0.1 * (number of articles)\n    \"\"\"\n    df = to_pandas(df).copy()\n    \n    # Impute missing lists with empty lists:\n    df['days_before_lst'] = df['days_before_lst'].apply(lambda x: x if isinstance(x, list) else [] if pd.isnull(x) else x)\n    df['articles_ids_lst'] = df['articles_ids_lst'].apply(lambda x: x if isinstance(x, list) else [] if pd.isnull(x) else x)\n    # Impute missing regression_label with 0\n    df['regression_label'] = df['regression_label'].fillna(0)\n    \n    frequencies, recencies, Ts, monetary_values = [], [], [], []\n    \n    for idx, row in df.iterrows():\n        days = row['days_before_lst']\n        if not days or len(days)==0:\n            frequencies.append(0)\n            recencies.append(0.0)\n            Ts.append(0.0)\n        else:\n            x = len(days)\n            frequencies.append(max(x-1, 0))\n            recencies.append(days[-1] - days[0])\n            Ts.append(days[-1] - days[0])\n        arts = row['articles_ids_lst']\n        if not arts or len(arts)==0:\n            monetary_values.append(20.0)\n        else:\n            monetary_values.append(20.0 + 0.1 * len(arts))\n    \n    features = pd.DataFrame({\n        'frequency': frequencies,\n        'recency': recencies,\n        'T': Ts,\n        'monetary_value': monetary_values,\n        'regression_label': df['regression_label']\n    })\n    return features\n\n\ndef predict_future_transactions(fitter, frequency, recency, T, t_future):\n    \"\"\"\n    Compute the expected number of future transactions over a period t_future:\n      E[X(t_future)] = p_alive * ((r + frequency) / (alpha + T)) * t_future\n    Here, p_alive is obtained via fitter.conditional_probability_alive.\n    This method is available in BetaGeoFitter, and we assume ParetoNBDFitter and MBGNBDFitter\n    mimic the same API.\n    \"\"\"\n    p_alive = fitter.conditional_probability_alive(frequency, recency, T)\n    return p_alive * ((fitter.r_ + frequency) / (fitter.alpha_ + T)) * t_future\n\ndef predict_monetary_value(ggf, frequency, monetary_value):\n    \"\"\"\n    Compute the expected average monetary value:\n      E[m] = (p + q*frequency) / (v + frequency * monetary_value)\n    \"\"\"\n    return (ggf.p_ + ggf.q_ * frequency) / (ggf.v_ + frequency * monetary_value + 1e-8)\n\ndef compute_predicted_clv(fitter, ggf, features_df, t_future):\n    \"\"\"\n    For each record, compute:\n       predicted_CLV = E[X(t_future)] * E[m]\n    \"\"\"\n    predictions = []\n    for idx, row in features_df.iterrows():\n        freq = row['frequency']\n        rec = row['recency']\n        T_val = row['T']\n        mon_val = row['monetary_value']\n        exp_tx = predict_future_transactions(fitter, freq, rec, T_val, t_future)\n        exp_m = predict_monetary_value(ggf, freq, mon_val)\n        predictions.append(exp_tx * exp_m)\n    features_df = features_df.copy()\n    features_df['predicted_clv'] = predictions\n    return features_df\n\n\ndef main_pipeline(train_df, val_df, test_df, t_future=12, discount_rate=0.01):\n    \"\"\"\n    For each model variant (BG–NBD, Pareto–NBD, MBG/NBD) calibrate on train_df,\n    then compute predicted CLV on train, validation, and test sets.\n    Extra columns (like customer_id, age, postal_code) are ignored.\n    \"\"\"\n    # Extract features (with imputation)\n    train_features = impute_and_extract_features(train_df)\n    val_features = impute_and_extract_features(val_df)\n    test_features = impute_and_extract_features(test_df)\n    \n    results = {}\n    \n    # 1. BG–NBD + Gamma–Gamma using lifetimes’ BetaGeoFitter and GammaGammaFitter\n    bgf = BetaGeoFitter(penalizer_coef=0.0)\n    bgf.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    # Fit Gamma–Gamma on customers with frequency > 0\n    mask = train_features['frequency'] > 0\n    ggf_bg = GammaGammaFitter(penalizer_coef=0.0)\n    ggf_bg.fit(train_features.loc[mask, 'monetary_value'], train_features.loc[mask, 'frequency'])\n    results['BG-NBD'] = {\n        'train': compute_predicted_clv(bgf, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(bgf, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(bgf, ggf_bg, test_features, t_future)\n    }\n    \n    # 2. Pareto–NBD + Gamma–Gamma using lifetimes’ ParetoNBDFitter and GammaGammaFitter\n    pareto_fitter = ParetoNBDFitter(penalizer_coef=0.0)\n    pareto_fitter.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    results['Pareto-NBD'] = {\n        'train': compute_predicted_clv(pareto_fitter, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(pareto_fitter, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(pareto_fitter, ggf_bg, test_features, t_future)\n    }\n    \n    # 3. MBG/NBD + Gamma–Gamma using lifetimes’ MBGNBDFitter and GammaGammaFitter\n    mbg_fitter = MBGNBDFitter(penalizer_coef=0.0)\n    mbg_fitter.fit(train_features['frequency'], train_features['recency'], train_features['T'])\n    results['MBG-NBD'] = {\n        'train': compute_predicted_clv(mbg_fitter, ggf_bg, train_features, t_future),\n        'val': compute_predicted_clv(mbg_fitter, ggf_bg, val_features, t_future),\n        'test': compute_predicted_clv(mbg_fitter, ggf_bg, test_features, t_future)\n    }\n    \n    return results\n\n\n\nresults = main_pipeline(train_df, val_df, test_df, t_future=12)\n    \nprint(\"=== BG–NBD + Gamma–Gamma Predictions (Test) ===\")\nprint(results['BG-NBD']['test'])\nprint(\"\\n=== Pareto–NBD + Gamma–Gamma Predictions (Test) ===\")\nprint(results['Pareto-NBD']['test'])\nprint(\"\\n=== MBG–NBD + Gamma–Gamma Predictions (Test) ===\")\nprint(results['MBG-NBD']['test'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}